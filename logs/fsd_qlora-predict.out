nohup: ignoring input
[2024-06-03 01:44:52,114] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
06/03/2024 01:44:53 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2106] 2024-06-03 01:44:53,516 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2106] 2024-06-03 01:44:53,516 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2106] 2024-06-03 01:44:53,516 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-06-03 01:44:53,516 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-06-03 01:44:53,516 >> loading file tokenizer_config.json
06/03/2024 01:44:53 - INFO - llamafactory.data.template - Add pad token: </s>
06/03/2024 01:44:53 - INFO - llamafactory.data.loader - Loading dataset AssetGeneration/test.json...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 68916 examples [00:00, 73653.88 examples/s]Generating train split: 68916 examples [00:00, 71359.87 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/5000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 5000/5000 [00:00<00:00, 27952.75 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/5000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 313/5000 [00:00<00:03, 1348.31 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 1565/5000 [00:00<00:00, 5438.89 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 3440/5000 [00:00<00:00, 10035.57 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 5000/5000 [00:00<00:00, 11318.27 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 5000/5000 [00:00<00:00, 7870.55 examples/s] 
[INFO|configuration_utils.py:731] 2024-06-03 01:44:56,276 >> loading configuration file /data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:796] 2024-06-03 01:44:56,277 >> Model config MistralConfig {
  "_name_or_path": "/data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

input_ids:
[1, 28705, 733, 16289, 28793, 5919, 8270, 28705, 28770, 1964, 10220, 297, 4300, 3842, 28725, 2818, 356, 272, 2296, 1871, 28747, 13, 13, 17500, 5140, 28747, 4449, 1508, 2849, 28723, 3423, 1466, 16210, 404, 28723, 675, 28748, 28726, 7041, 28733, 2031, 288, 28748, 8830, 2951, 28733, 28726, 24681, 28723, 2842, 28804, 3457, 28770, 28746, 28740, 28734, 28734, 28784, 28782, 28774, 28800, 3457, 28782, 28746, 28750, 28782, 28740, 28787, 28782, 28800, 3457, 28770, 28770, 28770, 28746, 28750, 28782, 28740, 28774, 28784, 28800, 3457, 28770, 28770, 28784, 28746, 28750, 28782, 28740, 28787, 28750, 28800, 3457, 28770, 28770, 28787, 28746, 28750, 28782, 28740, 28787, 28770, 28800, 3457, 28770, 28770, 28783, 28746, 28750, 28782, 28740, 28787, 28781, 28800, 3457, 28770, 28781, 28783, 28746, 28770, 28782, 28782, 28783, 28740, 28800, 5646, 28730, 313, 28746, 28781, 28783, 28781, 28800, 23114, 28730, 9974, 28746, 11155, 28705, 13, 12075, 28747, 1679, 1466, 16210, 404, 28723, 675, 28705, 13, 9633, 28747, 365, 28750, 28743, 8074, 1939, 12860, 8074, 1939, 13281, 288, 567, 26999, 28705, 13, 22971, 288, 4068, 28747, 28705, 842, 12670, 2951, 365, 24681, 387, 7973, 7809, 1962, 365, 7041, 13281, 288, 342, 8580, 12422, 28765, 346, 404, 28745, 28705, 842, 7497, 27074, 28735, 842, 16123, 28790, 22285, 842, 12670, 2951, 365, 24681, 842, 13341, 13281, 19641, 842, 21013, 25181, 842, 12670, 2951, 365, 24681, 28747, 12948, 1590, 4922, 2956, 297, 4922, 816, 1223, 842, 21815, 272, 24443, 13909, 12670, 2951, 9315, 28713, 304, 365, 24681, 842, 8420, 522, 12670, 2951, 15210, 354, 1756, 11019, 442, 4655, 9405, 5938, 842, 382, 6185, 16970, 18945, 842, 26406, 354, 8648, 288, 16782, 495, 12670, 2951, 365, 24681, 842, 2483, 680, 10636, 28747, 842, 995, 993, 835, 737, 28747, 842, 28705, 13, 15962, 13063, 28747, 1444, 28705, 28783, 28781, 298, 28705, 28774, 28734, 6128, 28723, 28705, 13, 733, 28748, 16289, 28793]
inputs:
<s>  [INST] Please generate 3 Ad Description in English language, based on the following information:

FinalUrl: https://www.nextdayflyers.com/banner-printing/vinyl-banners.php?attr3=100659&attr5=25175&attr333=25196&attr336=25172&attr337=25173&attr338=25174&attr348=35581&product_id=484&calc_variant=LIST 
Domain: nextdayflyers.com 
Category: B2C Services -- Personal Services -- Printing & Publishing 
LandingPage:  . Vinyl Banners - Custom Premium Banner Printing | NextDayFlyers;  . PRODUCTS . SERVICES . Vinyl Banners . Select Print Options . Overall Rating . Vinyl Banners: Promote Anywhere in Any Weather . Choose the Perfect Size Vinyl Signs and Banners . Durable Vinyl Material for Indoor or Outdoor Use . Hanging Made Easy . Tips for Designing Effective Vinyl Banners . Get more tips: . You may also like: . 
CharacterLimit: between 84 to 90 characters. 
 [/INST]
06/03/2024 01:44:56 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
[INFO|modeling_utils.py:3460] 2024-06-03 01:44:56,289 >> loading weights file /data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2/model.safetensors.index.json
[INFO|modeling_utils.py:1508] 2024-06-03 01:44:56,289 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:962] 2024-06-03 01:44:56,290 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.00s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.30it/s]
[INFO|modeling_utils.py:4269] 2024-06-03 01:44:58,788 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|modeling_utils.py:4277] 2024-06-03 01:44:58,788 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|configuration_utils.py:915] 2024-06-03 01:44:58,790 >> loading configuration file /data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2/generation_config.json
[INFO|configuration_utils.py:962] 2024-06-03 01:44:58,790 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

06/03/2024 01:44:58 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.
06/03/2024 01:44:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
06/03/2024 01:44:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
06/03/2024 01:44:59 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).
06/03/2024 01:44:59 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/mistral/fsdp_qlora_sft
06/03/2024 01:44:59 - INFO - llamafactory.model.loader - all params: 7241732096
[INFO|trainer.py:3719] 2024-06-03 01:44:59,311 >> ***** Running Prediction *****
[INFO|trainer.py:3721] 2024-06-03 01:44:59,311 >>   Num examples = 5000
[INFO|trainer.py:3724] 2024-06-03 01:44:59,311 >>   Batch size = 28
  0%|          | 0/179 [00:00<?, ?it/s]  1%|          | 2/179 [00:22<33:44, 11.44s/it]  2%|▏         | 3/179 [00:40<40:33, 13.83s/it]  2%|▏         | 4/179 [00:56<43:10, 14.80s/it]  3%|▎         | 5/179 [01:07<39:32, 13.63s/it]  3%|▎         | 6/179 [01:33<50:25, 17.49s/it]  4%|▍         | 7/179 [01:45<45:06, 15.74s/it]  4%|▍         | 8/179 [01:59<43:05, 15.12s/it]  5%|▌         | 9/179 [02:18<46:45, 16.50s/it]  6%|▌         | 10/179 [02:34<45:53, 16.29s/it]  6%|▌         | 11/179 [02:45<41:03, 14.66s/it]  7%|▋         | 12/179 [03:01<42:26, 15.25s/it]  7%|▋         | 13/179 [03:16<41:36, 15.04s/it]  8%|▊         | 14/179 [03:39<48:13, 17.54s/it]  8%|▊         | 15/179 [03:59<49:19, 18.05s/it]  9%|▉         | 16/179 [04:15<47:50, 17.61s/it]  9%|▉         | 17/179 [04:33<47:25, 17.57s/it] 10%|█         | 18/179 [04:46<44:04, 16.42s/it] 11%|█         | 19/179 [05:01<42:32, 15.95s/it] 11%|█         | 20/179 [05:18<42:43, 16.12s/it] 12%|█▏        | 21/179 [05:37<44:52, 17.04s/it] 12%|█▏        | 22/179 [05:52<42:47, 16.35s/it] 13%|█▎        | 23/179 [06:05<40:25, 15.55s/it] 13%|█▎        | 24/179 [06:23<41:52, 16.21s/it] 14%|█▍        | 25/179 [06:36<39:22, 15.34s/it] 15%|█▍        | 26/179 [06:50<37:28, 14.70s/it] 15%|█▌        | 27/179 [07:07<39:16, 15.50s/it] 16%|█▌        | 28/179 [07:26<41:59, 16.68s/it] 16%|█▌        | 29/179 [07:51<47:44, 19.10s/it] 17%|█▋        | 30/179 [08:13<49:06, 19.78s/it] 17%|█▋        | 31/179 [08:26<44:24, 18.01s/it] 18%|█▊        | 32/179 [08:43<43:25, 17.73s/it] 18%|█▊        | 33/179 [09:07<47:27, 19.51s/it] 19%|█▉        | 34/179 [09:30<49:32, 20.50s/it] 20%|█▉        | 35/179 [09:41<42:41, 17.79s/it] 20%|██        | 36/179 [09:59<42:12, 17.71s/it] 21%|██        | 37/179 [10:21<44:53, 18.97s/it] 21%|██        | 38/179 [10:43<47:00, 20.01s/it] 22%|██▏       | 39/179 [11:02<45:35, 19.54s/it] 22%|██▏       | 40/179 [11:26<48:19, 20.86s/it] 23%|██▎       | 41/179 [11:41<43:56, 19.10s/it] 23%|██▎       | 42/179 [12:03<45:39, 20.00s/it] 24%|██▍       | 43/179 [12:26<47:34, 20.99s/it] 25%|██▍       | 44/179 [12:38<41:17, 18.35s/it] 25%|██▌       | 45/179 [12:52<37:58, 17.01s/it] 26%|██▌       | 46/179 [13:18<43:43, 19.73s/it] 26%|██▋       | 47/179 [13:29<37:31, 17.06s/it] 27%|██▋       | 48/179 [13:48<38:32, 17.65s/it] 27%|██▋       | 49/179 [14:06<38:41, 17.86s/it] 28%|██▊       | 50/179 [14:22<37:07, 17.27s/it] 28%|██▊       | 51/179 [14:36<34:47, 16.31s/it] 29%|██▉       | 52/179 [14:55<35:58, 17.00s/it] 30%|██▉       | 53/179 [15:13<36:14, 17.26s/it] 30%|███       | 54/179 [15:31<36:28, 17.51s/it] 31%|███       | 55/179 [15:50<37:13, 18.01s/it] 31%|███▏      | 56/179 [16:15<41:04, 20.04s/it] 32%|███▏      | 57/179 [16:32<39:02, 19.20s/it] 32%|███▏      | 58/179 [16:45<35:02, 17.38s/it] 33%|███▎      | 59/179 [16:59<32:41, 16.34s/it] 34%|███▎      | 60/179 [18:38<1:21:44, 41.21s/it] 34%|███▍      | 61/179 [18:57<1:07:51, 34.50s/it] 35%|███▍      | 62/179 [19:11<55:14, 28.33s/it]   35%|███▌      | 63/179 [19:26<46:48, 24.21s/it] 36%|███▌      | 64/179 [19:37<38:44, 20.22s/it] 36%|███▋      | 65/179 [19:56<38:10, 20.10s/it] 37%|███▋      | 66/179 [20:14<36:34, 19.42s/it] 37%|███▋      | 67/179 [20:46<43:22, 23.24s/it] 38%|███▊      | 68/179 [20:58<36:20, 19.64s/it] 39%|███▊      | 69/179 [21:14<34:14, 18.68s/it] 39%|███▉      | 70/179 [21:31<32:42, 18.00s/it] 40%|███▉      | 71/179 [21:47<31:28, 17.48s/it] 40%|████      | 72/179 [22:12<35:04, 19.66s/it] 41%|████      | 73/179 [22:21<29:10, 16.51s/it] 41%|████▏     | 74/179 [22:34<27:19, 15.62s/it] 42%|████▏     | 75/179 [22:53<28:35, 16.50s/it] 42%|████▏     | 76/179 [23:10<28:43, 16.73s/it] 43%|████▎     | 77/179 [23:25<27:40, 16.28s/it] 44%|████▎     | 78/179 [23:47<30:12, 17.94s/it] 44%|████▍     | 79/179 [24:09<31:51, 19.11s/it] 45%|████▍     | 80/179 [24:29<31:59, 19.39s/it] 45%|████▌     | 81/179 [24:45<29:59, 18.37s/it] 46%|████▌     | 82/179 [25:07<31:25, 19.44s/it] 46%|████▋     | 83/179 [25:24<29:56, 18.72s/it] 47%|████▋     | 84/179 [25:43<29:36, 18.70s/it] 47%|████▋     | 85/179 [25:55<26:27, 16.89s/it] 48%|████▊     | 86/179 [26:09<24:42, 15.94s/it] 49%|████▊     | 87/179 [26:27<25:27, 16.60s/it] 49%|████▉     | 88/179 [26:45<25:35, 16.88s/it] 50%|████▉     | 89/179 [27:04<26:30, 17.67s/it] 50%|█████     | 90/179 [27:20<25:11, 16.98s/it] 51%|█████     | 91/179 [27:37<24:55, 17.00s/it] 51%|█████▏    | 92/179 [27:49<22:24, 15.46s/it] 52%|█████▏    | 93/179 [28:08<23:54, 16.69s/it] 53%|█████▎    | 94/179 [28:24<23:13, 16.40s/it] 53%|█████▎    | 95/179 [28:40<22:40, 16.19s/it] 54%|█████▎    | 96/179 [28:55<22:09, 16.02s/it] 54%|█████▍    | 97/179 [29:17<24:09, 17.67s/it] 55%|█████▍    | 98/179 [29:34<23:44, 17.58s/it] 55%|█████▌    | 99/179 [29:52<23:38, 17.74s/it] 56%|█████▌    | 100/179 [30:11<23:51, 18.11s/it] 56%|█████▋    | 101/179 [30:24<21:25, 16.48s/it] 57%|█████▋    | 102/179 [30:57<27:44, 21.62s/it] 58%|█████▊    | 103/179 [31:07<22:54, 18.08s/it] 58%|█████▊    | 104/179 [31:22<21:19, 17.06s/it] 59%|█████▊    | 105/179 [31:38<20:34, 16.68s/it] 59%|█████▉    | 106/179 [31:57<21:21, 17.56s/it] 60%|█████▉    | 107/179 [32:14<20:53, 17.41s/it] 60%|██████    | 108/179 [33:09<33:46, 28.54s/it] 61%|██████    | 109/179 [33:32<31:26, 26.95s/it] 61%|██████▏   | 110/179 [33:50<27:59, 24.34s/it] 62%|██████▏   | 111/179 [34:03<23:39, 20.87s/it] 63%|██████▎   | 112/179 [34:20<21:54, 19.62s/it] 63%|██████▎   | 113/179 [34:38<21:03, 19.15s/it] 64%|██████▎   | 114/179 [34:54<19:50, 18.31s/it] 64%|██████▍   | 115/179 [35:10<18:39, 17.49s/it] 65%|██████▍   | 116/179 [35:31<19:40, 18.73s/it] 65%|██████▌   | 117/179 [35:41<16:35, 16.05s/it] 66%|██████▌   | 118/179 [36:01<17:18, 17.02s/it] 66%|██████▋   | 119/179 [36:23<18:37, 18.63s/it] 67%|██████▋   | 120/179 [36:40<17:56, 18.25s/it] 68%|██████▊   | 121/179 [36:58<17:30, 18.11s/it] 68%|██████▊   | 122/179 [37:11<15:35, 16.41s/it] 69%|██████▊   | 123/179 [37:27<15:26, 16.54s/it] 69%|██████▉   | 124/179 [37:48<16:13, 17.71s/it] 70%|██████▉   | 125/179 [38:08<16:44, 18.60s/it] 70%|███████   | 126/179 [38:27<16:23, 18.56s/it] 71%|███████   | 127/179 [38:41<14:51, 17.14s/it] 72%|███████▏  | 128/179 [39:00<15:00, 17.65s/it] 72%|███████▏  | 129/179 [39:23<16:08, 19.38s/it] 73%|███████▎  | 130/179 [39:35<13:59, 17.14s/it] 73%|███████▎  | 131/179 [39:59<15:23, 19.24s/it] 74%|███████▎  | 132/179 [40:27<17:05, 21.82s/it] 74%|███████▍  | 133/179 [41:03<20:02, 26.13s/it] 75%|███████▍  | 134/179 [41:19<17:12, 22.93s/it] 75%|███████▌  | 135/179 [41:35<15:27, 21.08s/it] 76%|███████▌  | 136/179 [41:50<13:39, 19.06s/it] 77%|███████▋  | 137/179 [42:02<11:58, 17.10s/it] 77%|███████▋  | 138/179 [42:13<10:29, 15.35s/it] 78%|███████▊  | 139/179 [42:34<11:10, 16.77s/it] 78%|███████▊  | 140/179 [42:47<10:19, 15.88s/it] 79%|███████▉  | 141/179 [43:06<10:39, 16.82s/it] 79%|███████▉  | 142/179 [43:26<10:58, 17.79s/it] 80%|███████▉  | 143/179 [43:45<10:51, 18.11s/it] 80%|████████  | 144/179 [44:00<09:58, 17.11s/it] 81%|████████  | 145/179 [44:24<10:55, 19.27s/it] 82%|████████▏ | 146/179 [44:44<10:43, 19.50s/it] 82%|████████▏ | 147/179 [45:06<10:41, 20.05s/it] 83%|████████▎ | 148/179 [45:21<09:38, 18.65s/it] 83%|████████▎ | 149/179 [45:32<08:13, 16.44s/it] 84%|████████▍ | 150/179 [45:43<07:06, 14.72s/it] 84%|████████▍ | 151/179 [46:04<07:44, 16.60s/it] 85%|████████▍ | 152/179 [46:16<06:48, 15.13s/it] 85%|████████▌ | 153/179 [46:26<05:57, 13.74s/it] 86%|████████▌ | 154/179 [46:41<05:50, 14.03s/it] 87%|████████▋ | 155/179 [47:04<06:43, 16.80s/it] 87%|████████▋ | 156/179 [47:13<05:30, 14.38s/it] 88%|████████▊ | 157/179 [47:28<05:22, 14.67s/it] 88%|████████▊ | 158/179 [47:41<04:55, 14.07s/it] 89%|████████▉ | 159/179 [47:59<05:07, 15.35s/it] 89%|████████▉ | 160/179 [48:13<04:43, 14.95s/it] 90%|████████▉ | 161/179 [48:28<04:28, 14.93s/it] 91%|█████████ | 162/179 [48:43<04:11, 14.81s/it] 91%|█████████ | 163/179 [49:03<04:21, 16.36s/it] 92%|█████████▏| 164/179 [49:20<04:09, 16.64s/it] 92%|█████████▏| 165/179 [49:39<04:04, 17.47s/it] 93%|█████████▎| 166/179 [49:52<03:28, 16.06s/it] 93%|█████████▎| 167/179 [50:19<03:51, 19.27s/it] 94%|█████████▍| 168/179 [50:38<03:32, 19.30s/it] 94%|█████████▍| 169/179 [50:58<03:13, 19.36s/it] 95%|█████████▍| 170/179 [51:11<02:38, 17.57s/it] 96%|█████████▌| 171/179 [51:30<02:22, 17.87s/it] 96%|█████████▌| 172/179 [52:03<02:37, 22.52s/it] 97%|█████████▋| 173/179 [52:14<01:53, 18.86s/it] 97%|█████████▋| 174/179 [52:26<01:24, 17.00s/it] 98%|█████████▊| 175/179 [52:40<01:04, 16.15s/it] 98%|█████████▊| 176/179 [52:58<00:49, 16.59s/it] 99%|█████████▉| 177/179 [53:23<00:38, 19.01s/it] 99%|█████████▉| 178/179 [53:47<00:20, 20.66s/it]100%|██████████| 179/179 [53:55<00:00, 16.74s/it]Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.499 seconds.
Prefix dict has been built successfully.
100%|██████████| 179/179 [54:05<00:00, 18.13s/it]
***** predict metrics *****
  predict_bleu-4             =    41.5635
  predict_rouge-1            =    44.0986
  predict_rouge-2            =    26.7084
  predict_rouge-l            =    42.0319
  predict_runtime            = 0:54:19.39
  predict_samples_per_second =      1.534
  predict_steps_per_second   =      0.055
06/03/2024 02:39:18 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/mistral/fsdp_qlora_sft/predict/generated_predictions.jsonl
