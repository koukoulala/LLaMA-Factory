nohup: ignoring input
[2024-07-07 03:10:00,921] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
07/07/2024 03:10:02 - WARNING - llamafactory.hparams.parser - Evaluating model in 4/8-bit mode may cause lower scores.
07/07/2024 03:10:02 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:10:02,327 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:10:02,327 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:10:02,327 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:10:02,327 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:10:02,327 >> loading file tokenizer_config.json
07/07/2024 03:10:02 - INFO - llamafactory.data.template - Add pad token: </s>
07/07/2024 03:10:02 - INFO - llamafactory.data.loader - Loading dataset AssetGeneration/test_AddDiversity.json...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 68916 examples [00:00, 89618.12 examples/s]Generating train split: 68916 examples [00:00, 86393.67 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/5000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 5000/5000 [00:00<00:00, 29054.88 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/5000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 313/5000 [00:00<00:03, 1266.51 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 2191/5000 [00:00<00:00, 7449.01 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 4064/5000 [00:00<00:00, 11204.58 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 5000/5000 [00:00<00:00, 8492.32 examples/s] 
[INFO|configuration_utils.py:731] 2024-07-07 03:10:04,735 >> loading configuration file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:796] 2024-07-07 03:10:04,736 >> Model config MistralConfig {
  "_name_or_path": "/data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

input_ids:
[1, 28705, 733, 16289, 28793, 5919, 8270, 28705, 28787, 1964, 9655, 1081, 297, 4300, 3842, 28725, 2818, 356, 272, 2296, 1871, 28747, 13, 13, 17500, 5140, 28747, 4449, 1508, 2849, 28723, 17755, 1468, 2230, 28723, 675, 28748, 28768, 321, 1916, 28733, 28768, 1953, 28713, 28733, 2222, 28733, 17755, 1468, 28748, 8008, 28705, 13, 12075, 28747, 15551, 2230, 28723, 675, 28705, 13, 9633, 28747, 20214, 28705, 13, 22971, 288, 4068, 28747, 28705, 842, 15908, 2215, 28713, 8246, 320, 25655, 387, 320, 10106, 28735, 2230, 28723, 675, 28745, 28705, 842, 15908, 2215, 28713, 8246, 28745, 28705, 842, 320, 25655, 842, 15908, 2215, 28713, 8246, 320, 25655, 1418, 19338, 842, 3797, 2205, 394, 796, 346, 351, 6461, 999, 28713, 438, 13830, 1286, 935, 294, 1334, 842, 5691, 2205, 25191, 15406, 4837, 438, 25613, 23451, 2222, 1739, 28708, 740, 842, 13830, 1286, 935, 294, 1334, 438, 3797, 2205, 394, 796, 346, 351, 6461, 999, 28713, 842, 25613, 23451, 2222, 1739, 28708, 740, 438, 3797, 2205, 394, 796, 346, 351, 6461, 999, 28713, 842, 13830, 1286, 935, 294, 1334, 438, 5691, 2205, 25191, 15406, 4837, 842, 13830, 1286, 935, 294, 1334, 438, 25613, 365, 28705, 13, 15962, 13063, 28747, 1444, 28705, 28740, 28734, 298, 28705, 28770, 28734, 6128, 28723, 28705, 13, 17621, 454, 28747, 25217, 18644, 486, 12144, 288, 4118, 10158, 284, 296, 1074, 297, 1430, 1335, 1081, 28723, 28705, 13, 733, 28748, 16289, 28793]
inputs:
<s>  [INST] Please generate 7 Ad Headline in English language, based on the following information:

FinalUrl: https://www.ticketsales.com/jimmy-johns-field-tickets/venue 
Domain: ticketsales.com 
Category: Entertainment 
LandingPage:  . Jimmy Johns Field Tickets - TicketSales.com;  . Jimmy Johns Field;  . Tickets . Jimmy Johns Field Tickets On Sale . Westside Woolly Mammoths at Utica Unicorn . Eastside Diamond Hoppers at Birmingham Bloomfield Beavers . Utica Unicorn at Westside Woolly Mammoths . Birmingham Bloomfield Beavers at Westside Woolly Mammoths . Utica Unicorn at Eastside Diamond Hoppers . Utica Unicorn at Birmingham B 
CharacterLimit: between 10 to 30 characters. 
Insight: Ensure diversity by highlighting various selling pionts in each headline. 
 [/INST]
07/07/2024 03:10:04 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.
07/07/2024 03:10:04 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
[INFO|modeling_utils.py:3460] 2024-07-07 03:10:04,748 >> loading weights file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/model.safetensors.index.json
[INFO|modeling_utils.py:1508] 2024-07-07 03:10:04,749 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:962] 2024-07-07 03:10:04,749 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.20it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s]
[INFO|modeling_utils.py:4269] 2024-07-07 03:10:07,502 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|modeling_utils.py:4277] 2024-07-07 03:10:07,502 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|configuration_utils.py:915] 2024-07-07 03:10:07,504 >> loading configuration file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/generation_config.json
[INFO|configuration_utils.py:962] 2024-07-07 03:10:07,504 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

07/07/2024 03:10:07 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.
07/07/2024 03:10:07 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
07/07/2024 03:10:07 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/07/2024 03:10:07 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/mistral/orpo_qlora/
07/07/2024 03:10:07 - INFO - llamafactory.model.loader - all params: 7262703616
[INFO|trainer.py:3719] 2024-07-07 03:10:08,030 >> ***** Running Prediction *****
[INFO|trainer.py:3721] 2024-07-07 03:10:08,030 >>   Num examples = 5000
[INFO|trainer.py:3724] 2024-07-07 03:10:08,030 >>   Batch size = 20
  0%|          | 0/250 [00:00<?, ?it/s]  1%|          | 2/250 [00:28<58:07, 14.06s/it]  1%|          | 3/250 [00:56<1:22:48, 20.11s/it]  2%|▏         | 4/250 [03:10<4:15:43, 62.37s/it]  2%|▏         | 5/250 [03:39<3:28:24, 51.04s/it]  2%|▏         | 6/250 [04:00<2:46:35, 40.97s/it]  3%|▎         | 7/250 [06:01<4:29:33, 66.56s/it]  3%|▎         | 8/250 [06:25<3:34:39, 53.22s/it]  4%|▎         | 9/250 [08:32<5:06:00, 76.18s/it]