nohup: ignoring input
[2024-06-20 05:30:09,713] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/xiaoyukou/anaconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
06/20/2024 05:30:11 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
06/20/2024 05:30:11 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2106] 2024-06-20 05:30:11,096 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2106] 2024-06-20 05:30:11,096 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2106] 2024-06-20 05:30:11,096 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-06-20 05:30:11,096 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-06-20 05:30:11,096 >> loading file tokenizer_config.json
06/20/2024 05:30:11 - INFO - llamafactory.data.template - Add pad token: </s>
06/20/2024 05:30:11 - INFO - llamafactory.data.loader - Loading dataset AssetGeneration/train_orpo.json...
Converting format of dataset (num_proc=16):   0%|          | 0/77786 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  14%|â–ˆâ–        | 11000/77786 [00:00<00:00, 108551.56 examples/s]Converting format of dataset (num_proc=16):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 61586/77786 [00:00<00:00, 269679.70 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77786/77786 [00:00<00:00, 156914.92 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/77786 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   1%|â–         | 1000/77786 [00:00<01:07, 1142.85 examples/s]Running tokenizer on dataset (num_proc=16):   9%|â–‰         | 7000/77786 [00:00<00:07, 9212.69 examples/s]Running tokenizer on dataset (num_proc=16):  17%|â–ˆâ–‹        | 13000/77786 [00:01<00:03, 17267.21 examples/s]Running tokenizer on dataset (num_proc=16):  23%|â–ˆâ–ˆâ–Ž       | 18000/77786 [00:01<00:05, 11880.02 examples/s]Running tokenizer on dataset (num_proc=16):  31%|â–ˆâ–ˆâ–ˆ       | 24000/77786 [00:01<00:03, 17466.44 examples/s]Running tokenizer on dataset (num_proc=16):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 30000/77786 [00:01<00:02, 22829.55 examples/s]Running tokenizer on dataset (num_proc=16):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 35000/77786 [00:02<00:02, 14772.14 examples/s]Running tokenizer on dataset (num_proc=16):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 42000/77786 [00:02<00:01, 20329.51 examples/s]Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 49000/77786 [00:03<00:01, 14839.28 examples/s]Running tokenizer on dataset (num_proc=16):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 55000/77786 [00:03<00:01, 19099.84 examples/s]Running tokenizer on dataset (num_proc=16):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 60000/77786 [00:03<00:00, 22710.24 examples/s]Running tokenizer on dataset (num_proc=16):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 64862/77786 [00:04<00:00, 15518.33 examples/s]Running tokenizer on dataset (num_proc=16):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 70034/77786 [00:04<00:00, 19358.30 examples/s]Running tokenizer on dataset (num_proc=16):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 74342/77786 [00:04<00:00, 22476.88 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77786/77786 [00:08<00:00, 9395.21 examples/s] 
[INFO|configuration_utils.py:731] 2024-06-20 05:30:20,840 >> loading configuration file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:796] 2024-06-20 05:30:20,841 >> Model config MistralConfig {
  "_name_or_path": "/data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

chosen_input_ids:
[1, 28705, 733, 16289, 28793, 5919, 8270, 28705, 28740, 1964, 10220, 297, 4300, 3842, 28725, 2818, 356, 272, 2296, 1871, 28747, 13, 13, 17500, 5140, 28747, 4449, 1508, 3589, 311, 28723, 675, 28748, 28708, 28775, 28748, 26283, 28733, 2360, 28748, 2022, 28733, 16346, 271, 28733, 3669, 28748, 28705, 13, 12075, 28747, 290, 4812, 28723, 675, 28705, 13, 9633, 28747, 12511, 567, 9207, 15976, 10399, 1939, 6583, 28705, 13, 22971, 288, 4068, 28747, 28705, 842, 5938, 351, 4812, 28742, 28713, 13152, 7040, 298, 2968, 5766, 298, 1411, 28745, 28705, 842, 24786, 3376, 778, 574, 8021, 395, 28705, 28770, 28734, 28734, 28806, 710, 28733, 15248, 24906, 354, 9112, 19304, 28725, 6443, 1249, 23332, 28725, 2273, 11745, 28725, 304, 680, 28723, 28705, 13, 15962, 13063, 28747, 1444, 28705, 28783, 28787, 298, 28705, 28774, 28734, 6128, 28723, 28705, 13, 17621, 454, 28747, 4556, 3646, 935, 1651, 318, 3572, 1133, 1065, 2065, 28705, 13, 733, 28748, 16289, 28793, 1964, 28747, 28760, 699, 3376, 778, 574, 8021, 395, 28705, 28770, 28734, 28734, 28806, 4258, 28733, 15248, 24906, 354, 13152, 567, 13735, 4082, 28723, 13, 2]
chosen_inputs:
<s>  [INST] Please generate 1 Ad Description in English language, based on the following information:

FinalUrl: https://miro.com/aq/paid-search/map-creator-free/ 
Domain: miro.com 
Category: Technology & Telecommunications -- Software 
LandingPage:  . Use Miro's mapping tools to bring ideas to life;  . Bring everyone into your vision with 300+ pre-made templates for wireframes, organizational charts, mind maps, and more. 
CharacterLimit: between 87 to 90 characters. 
Insight: Highlight Unique Selling Propositions 
 [/INST] Ad:Bring everyone into your vision with 300+ Pre-made templates for mapping & diagramming.
</s>
chosen_label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1964, 28747, 28760, 699, 3376, 778, 574, 8021, 395, 28705, 28770, 28734, 28734, 28806, 4258, 28733, 15248, 24906, 354, 13152, 567, 13735, 4082, 28723, 13, 2]
chosen_labels:
Ad:Bring everyone into your vision with 300+ Pre-made templates for mapping & diagramming.
</s>
rejected_input_ids:
[1, 28705, 733, 16289, 28793, 5919, 8270, 28705, 28740, 1964, 10220, 297, 4300, 3842, 28725, 2818, 356, 272, 2296, 1871, 28747, 13, 13, 17500, 5140, 28747, 4449, 1508, 3589, 311, 28723, 675, 28748, 28708, 28775, 28748, 26283, 28733, 2360, 28748, 2022, 28733, 16346, 271, 28733, 3669, 28748, 28705, 13, 12075, 28747, 290, 4812, 28723, 675, 28705, 13, 9633, 28747, 12511, 567, 9207, 15976, 10399, 1939, 6583, 28705, 13, 22971, 288, 4068, 28747, 28705, 842, 5938, 351, 4812, 28742, 28713, 13152, 7040, 298, 2968, 5766, 298, 1411, 28745, 28705, 842, 24786, 3376, 778, 574, 8021, 395, 28705, 28770, 28734, 28734, 28806, 710, 28733, 15248, 24906, 354, 9112, 19304, 28725, 6443, 1249, 23332, 28725, 2273, 11745, 28725, 304, 680, 28723, 28705, 13, 15962, 13063, 28747, 1444, 28705, 28783, 28787, 298, 28705, 28774, 28734, 6128, 28723, 28705, 13, 17621, 454, 28747, 4556, 3646, 935, 1651, 318, 3572, 1133, 1065, 2065, 28705, 13, 733, 28748, 16289, 28793, 1964, 28747, 12995, 28733, 2098, 5284, 13, 2]
rejected_inputs:
<s>  [INST] Please generate 1 Ad Description in English language, based on the following information:

FinalUrl: https://miro.com/aq/paid-search/map-creator-free/ 
Domain: miro.com 
Category: Technology & Telecommunications -- Software 
LandingPage:  . Use Miro's mapping tools to bring ideas to life;  . Bring everyone into your vision with 300+ pre-made templates for wireframes, organizational charts, mind maps, and more. 
CharacterLimit: between 87 to 90 characters. 
Insight: Highlight Unique Selling Propositions 
 [/INST] Ad:Real-Time Data
</s>
rejected_label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1964, 28747, 12995, 28733, 2098, 5284, 13, 2]
rejected_labels:
Ad:Real-Time Data
</s>
06/20/2024 05:30:20 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.
[INFO|modeling_utils.py:3460] 2024-06-20 05:30:20,854 >> loading weights file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/model.safetensors.index.json
[INFO|modeling_utils.py:1508] 2024-06-20 05:30:20,854 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:962] 2024-06-20 05:30:20,855 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.20it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.28it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.36it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.33it/s]
[INFO|modeling_utils.py:4269] 2024-06-20 05:30:23,468 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|modeling_utils.py:4277] 2024-06-20 05:30:23,469 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|configuration_utils.py:915] 2024-06-20 05:30:23,471 >> loading configuration file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/generation_config.json
[INFO|configuration_utils.py:962] 2024-06-20 05:30:23,471 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

06/20/2024 05:30:23 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.
06/20/2024 05:30:23 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.
06/20/2024 05:30:23 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
06/20/2024 05:30:23 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
06/20/2024 05:30:23 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/mistral/fsdp_qlora_sft/
06/20/2024 05:30:23 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 7262703616 || trainable%: 0.2888
[INFO|trainer.py:641] 2024-06-20 05:30:23,993 >> Using auto half precision backend
06/20/2024 05:30:23 - WARNING - llamafactory.extras.callbacks - Previous trainer log in this folder will be deleted.
[INFO|trainer.py:2078] 2024-06-20 05:30:24,175 >> ***** Running training *****
[INFO|trainer.py:2079] 2024-06-20 05:30:24,175 >>   Num examples = 77,397
[INFO|trainer.py:2080] 2024-06-20 05:30:24,175 >>   Num Epochs = 1
[INFO|trainer.py:2081] 2024-06-20 05:30:24,175 >>   Instantaneous batch size per device = 18
[INFO|trainer.py:2084] 2024-06-20 05:30:24,175 >>   Total train batch size (w. parallel, distributed & accumulation) = 36
[INFO|trainer.py:2085] 2024-06-20 05:30:24,175 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2086] 2024-06-20 05:30:24,175 >>   Total optimization steps = 2,150
[INFO|trainer.py:2087] 2024-06-20 05:30:24,178 >>   Number of trainable parameters = 20,971,520
  0%|          | 0/2150 [00:00<?, ?it/s]  0%|          | 1/2150 [00:19<11:33:50, 19.37s/it]