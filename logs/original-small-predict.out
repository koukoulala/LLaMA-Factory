nohup: ignoring input
[2024-05-26 08:47:14,235] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/26/2024 08:47:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2106] 2024-05-26 08:47:15,660 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2106] 2024-05-26 08:47:15,660 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2106] 2024-05-26 08:47:15,660 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-05-26 08:47:15,660 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-05-26 08:47:15,660 >> loading file tokenizer_config.json
05/26/2024 08:47:15 - INFO - llamafactory.data.template - Add pad token: </s>
05/26/2024 08:47:15 - INFO - llamafactory.data.loader - Loading dataset AssetGeneration/small_test.json...
Converting format of dataset (num_proc=16):   0%|          | 0/200 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 200/200 [00:00<00:00, 1218.16 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/200 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 13/200 [00:00<00:01, 110.54 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 78/200 [00:00<00:00, 403.25 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 140/200 [00:00<00:00, 470.50 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 200/200 [00:00<00:00, 429.98 examples/s]
[INFO|configuration_utils.py:731] 2024-05-26 08:47:17,010 >> loading configuration file /data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:796] 2024-05-26 08:47:17,011 >> Model config MistralConfig {
  "_name_or_path": "/data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

input_ids:
[1, 28705, 733, 16289, 28793, 5919, 8270, 28705, 28770, 1964, 9655, 1081, 297, 4300, 3842, 28725, 2818, 356, 272, 2296, 1871, 28747, 13, 13, 17500, 5140, 28747, 4449, 1508, 2849, 28723, 20243, 21819, 28723, 675, 28748, 1237, 270, 411, 28748, 3253, 28733, 28708, 469, 262, 28748, 20243, 21819, 28733, 28713, 1881, 21387, 28733, 28719, 1109, 3611, 28705, 13, 12075, 28747, 7132, 21819, 28723, 675, 28705, 13, 9633, 28747, 20214, 1939, 20370, 567, 320, 25655, 1939, 23208, 415, 17244, 28705, 13, 22971, 288, 4068, 28747, 28705, 842, 23208, 19725, 560, 17870, 28725, 7826, 342, 17133, 21819, 3658, 21387, 351, 1109, 3611, 28745, 28705, 842, 17133, 21819, 3658, 21387, 351, 1109, 3611, 28745, 28705, 842, 8610, 4593, 842, 524, 969, 11857, 367, 5904, 28705, 28781, 842, 384, 1802, 28747, 2914, 6005, 842, 12185, 272, 4041, 842, 2236, 357, 3239, 842, 11357, 28712, 3494, 842, 7481, 393, 497, 365, 291, 13917, 842, 7409, 1471, 2047, 28747, 2387, 7481, 842, 22404, 3239, 3663, 1190, 842, 12623, 18748, 842, 5311, 433, 6353, 842, 27970, 15573, 842, 451, 15016, 24556, 28745, 28705, 842, 8784, 842, 320, 1139, 842, 7842, 842, 542, 1726, 842, 401, 373, 842, 10586, 842, 7057, 842, 3217, 842, 3301, 298, 11933, 3231, 842, 27793, 28705, 28740, 15384, 28705, 28770, 28781, 1187, 842, 1343, 28705, 13, 15962, 13063, 28747, 1444, 28705, 28740, 28734, 298, 28705, 28750, 28734, 6128, 28723, 28705, 13, 733, 28748, 16289, 28793]
inputs:
<s>  [INST] Please generate 3 Ad Headline in English language, based on the following information:

FinalUrl: https://www.cinemark.com/theatres/tx-austin/cinemark-southpark-meadows 
Domain: cinemark.com 
Category: Entertainment -- Events & Tickets -- Movie Theaters 
LandingPage:  . Movie Theater In Austin, Texas | Cinemark Southpark Meadows;  . Cinemark Southpark Meadows;  . Showtimes . Kung Fu Panda 4 . Dune: Part Two . Arthur the King . Imaginary . Cabrini . Love Lies Bleeding . Bob Marley: One Love . Ordinary Angels . Standard Format . Madame Web . Poor Things . Oppenheimer;  . Today . Tues . Wed . Thurs . Fri . Sat . Sun . Mon . Add to Watch List . PG 1 hr 34 min . De 
CharacterLimit: between 10 to 20 characters. 
 [/INST]
05/26/2024 08:47:17 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
[INFO|modeling_utils.py:3460] 2024-05-26 08:47:17,028 >> loading weights file /data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2/model.safetensors.index.json
[INFO|modeling_utils.py:1508] 2024-05-26 08:47:17,028 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:962] 2024-05-26 08:47:17,029 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.51it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.49it/s]
[INFO|modeling_utils.py:4269] 2024-05-26 08:47:19,274 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|modeling_utils.py:4277] 2024-05-26 08:47:19,274 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|configuration_utils.py:915] 2024-05-26 08:47:19,276 >> loading configuration file /data/xiaoyukou/alignment-handbook/ckpts/Mistral-7B-Instruct-v0.2/generation_config.json
[INFO|configuration_utils.py:962] 2024-05-26 08:47:19,276 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

05/26/2024 08:47:19 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.
05/26/2024 08:47:19 - INFO - llamafactory.model.adapter - Adapter is not found at evaluation, load the base model.
05/26/2024 08:47:19 - INFO - llamafactory.model.loader - all params: 7241732096
[INFO|trainer.py:3719] 2024-05-26 08:47:19,378 >> ***** Running Prediction *****
[INFO|trainer.py:3721] 2024-05-26 08:47:19,378 >>   Num examples = 200
[INFO|trainer.py:3724] 2024-05-26 08:47:19,378 >>   Batch size = 16
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:24<02:15, 12.30s/it] 23%|██▎       | 3/13 [00:52<03:09, 18.98s/it] 31%|███       | 4/13 [01:08<02:38, 17.63s/it] 38%|███▊      | 5/13 [01:36<02:49, 21.20s/it] 46%|████▌     | 6/13 [02:01<02:38, 22.61s/it] 54%|█████▍    | 7/13 [02:14<01:56, 19.37s/it] 62%|██████▏   | 8/13 [02:29<01:29, 17.99s/it] 69%|██████▉   | 9/13 [02:46<01:11, 17.92s/it] 77%|███████▋  | 10/13 [03:04<00:52, 17.67s/it] 85%|████████▍ | 11/13 [03:17<00:32, 16.38s/it] 92%|█████████▏| 12/13 [03:30<00:15, 15.33s/it]100%|██████████| 13/13 [03:43<00:00, 14.56s/it]Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.501 seconds.
Prefix dict has been built successfully.
100%|██████████| 13/13 [03:44<00:00, 17.26s/it]
***** predict metrics *****
  predict_bleu-4             =    16.1883
  predict_rouge-1            =    18.0344
  predict_rouge-2            =     3.5019
  predict_rouge-l            =    13.2036
  predict_runtime            = 0:03:57.25
  predict_samples_per_second =      0.843
  predict_steps_per_second   =      0.055
05/26/2024 08:51:16 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/mistral/original_small/predict/generated_predictions.jsonl
