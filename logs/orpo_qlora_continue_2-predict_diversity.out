nohup: ignoring input
[2024-07-07 03:11:03,724] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
07/07/2024 03:11:05 - WARNING - llamafactory.hparams.parser - Evaluating model in 4/8-bit mode may cause lower scores.
07/07/2024 03:11:05 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:11:05,124 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:11:05,124 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:11:05,124 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:11:05,124 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-07-07 03:11:05,124 >> loading file tokenizer_config.json
07/07/2024 03:11:05 - INFO - llamafactory.data.template - Add pad token: </s>
07/07/2024 03:11:05 - INFO - llamafactory.data.loader - Loading dataset AssetGeneration/test_AddDiversity.json...
Converting format of dataset (num_proc=16):   0%|          | 0/5000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 5000/5000 [00:00<00:00, 30641.21 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/5000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 313/5000 [00:00<00:03, 1359.80 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 2191/5000 [00:00<00:00, 7743.43 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 4064/5000 [00:00<00:00, 11496.64 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 5000/5000 [00:00<00:00, 8803.45 examples/s] 
[INFO|configuration_utils.py:731] 2024-07-07 03:11:06,620 >> loading configuration file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/config.json
[INFO|configuration_utils.py:796] 2024-07-07 03:11:06,620 >> Model config MistralConfig {
  "_name_or_path": "/data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

input_ids:
[1, 28705, 733, 16289, 28793, 5919, 8270, 28705, 28787, 1964, 9655, 1081, 297, 4300, 3842, 28725, 2818, 356, 272, 2296, 1871, 28747, 13, 13, 17500, 5140, 28747, 4449, 1508, 2849, 28723, 17755, 1468, 2230, 28723, 675, 28748, 28768, 321, 1916, 28733, 28768, 1953, 28713, 28733, 2222, 28733, 17755, 1468, 28748, 8008, 28705, 13, 12075, 28747, 15551, 2230, 28723, 675, 28705, 13, 9633, 28747, 20214, 28705, 13, 22971, 288, 4068, 28747, 28705, 842, 15908, 2215, 28713, 8246, 320, 25655, 387, 320, 10106, 28735, 2230, 28723, 675, 28745, 28705, 842, 15908, 2215, 28713, 8246, 28745, 28705, 842, 320, 25655, 842, 15908, 2215, 28713, 8246, 320, 25655, 1418, 19338, 842, 3797, 2205, 394, 796, 346, 351, 6461, 999, 28713, 438, 13830, 1286, 935, 294, 1334, 842, 5691, 2205, 25191, 15406, 4837, 438, 25613, 23451, 2222, 1739, 28708, 740, 842, 13830, 1286, 935, 294, 1334, 438, 3797, 2205, 394, 796, 346, 351, 6461, 999, 28713, 842, 25613, 23451, 2222, 1739, 28708, 740, 438, 3797, 2205, 394, 796, 346, 351, 6461, 999, 28713, 842, 13830, 1286, 935, 294, 1334, 438, 5691, 2205, 25191, 15406, 4837, 842, 13830, 1286, 935, 294, 1334, 438, 25613, 365, 28705, 13, 15962, 13063, 28747, 1444, 28705, 28740, 28734, 298, 28705, 28770, 28734, 6128, 28723, 28705, 13, 17621, 454, 28747, 25217, 18644, 486, 12144, 288, 4118, 10158, 284, 296, 1074, 297, 1430, 1335, 1081, 28723, 28705, 13, 733, 28748, 16289, 28793]
inputs:
<s>  [INST] Please generate 7 Ad Headline in English language, based on the following information:

FinalUrl: https://www.ticketsales.com/jimmy-johns-field-tickets/venue 
Domain: ticketsales.com 
Category: Entertainment 
LandingPage:  . Jimmy Johns Field Tickets - TicketSales.com;  . Jimmy Johns Field;  . Tickets . Jimmy Johns Field Tickets On Sale . Westside Woolly Mammoths at Utica Unicorn . Eastside Diamond Hoppers at Birmingham Bloomfield Beavers . Utica Unicorn at Westside Woolly Mammoths . Birmingham Bloomfield Beavers at Westside Woolly Mammoths . Utica Unicorn at Eastside Diamond Hoppers . Utica Unicorn at Birmingham B 
CharacterLimit: between 10 to 30 characters. 
Insight: Ensure diversity by highlighting various selling pionts in each headline. 
 [/INST]
07/07/2024 03:11:06 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.
07/07/2024 03:11:06 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.
[INFO|modeling_utils.py:3460] 2024-07-07 03:11:06,633 >> loading weights file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/model.safetensors.index.json
[INFO|modeling_utils.py:1508] 2024-07-07 03:11:06,634 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:962] 2024-07-07 03:11:06,634 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.18it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]
[INFO|modeling_utils.py:4269] 2024-07-07 03:11:09,275 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|modeling_utils.py:4277] 2024-07-07 03:11:09,275 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|configuration_utils.py:915] 2024-07-07 03:11:09,277 >> loading configuration file /data/xiaoyukou/ckpts/Mistral-7B-Instruct-v0.2/generation_config.json
[INFO|configuration_utils.py:962] 2024-07-07 03:11:09,277 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

07/07/2024 03:11:09 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.
07/07/2024 03:11:09 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
07/07/2024 03:11:09 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/07/2024 03:11:09 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/mistral/orpo_qlora_continue_2
07/07/2024 03:11:09 - INFO - llamafactory.model.loader - all params: 7262703616
[INFO|trainer.py:3719] 2024-07-07 03:11:09,880 >> ***** Running Prediction *****
[INFO|trainer.py:3721] 2024-07-07 03:11:09,881 >>   Num examples = 5000
[INFO|trainer.py:3724] 2024-07-07 03:11:09,881 >>   Batch size = 20
  0%|          | 0/250 [00:00<?, ?it/s]  1%|          | 2/250 [00:27<57:33, 13.93s/it]  1%|          | 3/250 [00:56<1:21:47, 19.87s/it]  2%|▏         | 4/250 [01:17<1:23:09, 20.28s/it]  2%|▏         | 5/250 [01:46<1:35:49, 23.47s/it]  2%|▏         | 6/250 [02:09<1:35:29, 23.48s/it]  3%|▎         | 7/250 [02:40<1:43:52, 25.65s/it]  3%|▎         | 8/250 [03:07<1:46:00, 26.28s/it]  4%|▎         | 9/250 [03:32<1:43:53, 25.87s/it]  4%|▍         | 10/250 [03:59<1:44:50, 26.21s/it]  4%|▍         | 11/250 [04:25<1:44:20, 26.19s/it]  5%|▍         | 12/250 [04:41<1:31:20, 23.03s/it]  5%|▌         | 13/250 [05:16<1:45:01, 26.59s/it]  6%|▌         | 14/250 [05:31<1:30:38, 23.04s/it]  6%|▌         | 15/250 [05:44<1:18:46, 20.11s/it]  6%|▋         | 16/250 [06:07<1:21:24, 20.87s/it]  7%|▋         | 17/250 [06:45<1:41:19, 26.09s/it]  7%|▋         | 18/250 [07:00<1:28:01, 22.76s/it]  8%|▊         | 19/250 [07:16<1:19:21, 20.61s/it]  8%|▊         | 20/250 [07:47<1:31:31, 23.88s/it]  8%|▊         | 21/250 [08:18<1:38:38, 25.84s/it]  9%|▉         | 22/250 [08:27<1:19:09, 20.83s/it]  9%|▉         | 23/250 [08:50<1:21:08, 21.45s/it]