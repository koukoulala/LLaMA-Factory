nohup: ignoring input
[2024-07-25 03:54:03,627] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-25 03:54:03,633] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/xiaoyukou/anaconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/xiaoyukou/anaconda3/envs/py3.10/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
07/25/2024 03:54:05 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
07/25/2024 03:54:05 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/25/2024 03:54:05 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2106] 2024-07-25 03:54:05,684 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2106] 2024-07-25 03:54:05,684 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2106] 2024-07-25 03:54:05,684 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2106] 2024-07-25 03:54:05,684 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-07-25 03:54:05,684 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-07-25 03:54:05,684 >> loading file tokenizer_config.json
07/25/2024 03:54:05 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
07/25/2024 03:54:05 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/25/2024 03:54:05 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[WARNING|logging.py:314] 2024-07-25 03:54:05,847 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/25/2024 03:54:05 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
07/25/2024 03:54:05 - INFO - llamafactory.data.loader - Loading dataset AssetGeneration/train.json...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/25/2024 03:54:05 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
Converting format of dataset (num_proc=16):   0%|          | 0/1000000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 5000/1000000 [00:00<00:23, 42258.63 examples/s]Converting format of dataset (num_proc=16):  16%|â–ˆâ–‹        | 163000/1000000 [00:00<00:00, 842205.42 examples/s]Converting format of dataset (num_proc=16):  35%|â–ˆâ–ˆâ–ˆâ–      | 346000/1000000 [00:00<00:00, 1222768.06 examples/s]Converting format of dataset (num_proc=16):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 528000/1000000 [00:00<00:00, 1430315.36 examples/s]Converting format of dataset (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 694000/1000000 [00:00<00:00, 1506787.36 examples/s]Converting format of dataset (num_proc=16):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 854000/1000000 [00:00<00:00, 1533887.37 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [00:03<00:00, 255216.70 examples/s]
07/25/2024 03:54:10 - INFO - llamafactory.data.loader - Loading dataset AssetGeneration/train.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 1000/1000000 [00:00<14:05, 1181.51 examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 3000/1000000 [00:00<04:34, 3636.09 examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 5000/1000000 [00:01<02:39, 6229.79 examples/s]Running tokenizer on dataset (num_proc=16):   1%|          | 7000/1000000 [00:01<02:06, 7836.06 examples/s]Running tokenizer on dataset (num_proc=16):   1%|          | 9000/1000000 [00:01<01:39, 9947.05 examples/s]Running tokenizer on dataset (num_proc=16):   1%|â–         | 13000/1000000 [00:01<01:01, 16068.70 examples/s]Running tokenizer on dataset (num_proc=16):   2%|â–         | 16000/1000000 [00:01<00:53, 18259.65 examples/s]Running tokenizer on dataset (num_proc=16):   2%|â–         | 19000/1000000 [00:01<00:47, 20523.90 examples/s]Running tokenizer on dataset (num_proc=16):   2%|â–         | 22000/1000000 [00:01<00:44, 21892.69 examples/s]Running tokenizer on dataset (num_proc=16):   2%|â–Ž         | 25000/1000000 [00:01<00:43, 22364.66 examples/s]Running tokenizer on dataset (num_proc=16):   3%|â–Ž         | 29000/1000000 [00:02<00:38, 25229.81 examples/s]Running tokenizer on dataset (num_proc=16):   3%|â–Ž         | 32000/1000000 [00:02<00:37, 25507.61 examples/s]Running tokenizer on dataset (num_proc=16):   4%|â–Ž         | 36000/1000000 [00:02<00:38, 24939.70 examples/s]Running tokenizer on dataset (num_proc=16):   4%|â–         | 41000/1000000 [00:02<00:36, 26302.97 examples/s]Running tokenizer on dataset (num_proc=16):   4%|â–         | 45000/1000000 [00:02<00:35, 27070.25 examples/s]Running tokenizer on dataset (num_proc=16):   5%|â–         | 48000/1000000 [00:02<00:34, 27555.29 examples/s]Running tokenizer on dataset (num_proc=16):   5%|â–Œ         | 51000/1000000 [00:02<00:33, 28071.93 examples/s]Running tokenizer on dataset (num_proc=16):   5%|â–Œ         | 54000/1000000 [00:03<00:35, 26501.23 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–Œ         | 57000/1000000 [00:03<00:34, 27144.82 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–Œ         | 61000/1000000 [00:03<00:36, 25945.12 examples/s]Running tokenizer on dataset (num_proc=16):   7%|â–‹         | 66000/1000000 [00:03<00:33, 28063.22 examples/s]Running tokenizer on dataset (num_proc=16):   7%|â–‹         | 69000/1000000 [00:03<00:33, 27616.20 examples/s]Running tokenizer on dataset (num_proc=16):   7%|â–‹         | 73000/1000000 [00:03<00:33, 27384.19 examples/s]Running tokenizer on dataset (num_proc=16):   8%|â–Š         | 77000/1000000 [00:03<00:35, 26159.47 examples/s]Running tokenizer on dataset (num_proc=16):   8%|â–Š         | 82000/1000000 [00:04<00:32, 28154.91 examples/s]Running tokenizer on dataset (num_proc=16):   8%|â–Š         | 85000/1000000 [00:04<00:33, 27364.01 examples/s]Running tokenizer on dataset (num_proc=16):   9%|â–‰         | 89000/1000000 [00:04<00:33, 27405.95 examples/s]Running tokenizer on dataset (num_proc=16):   9%|â–‰         | 93000/1000000 [00:04<00:34, 26114.26 examples/s]Running tokenizer on dataset (num_proc=16):  10%|â–‰         | 98000/1000000 [00:04<00:32, 27994.43 examples/s]Running tokenizer on dataset (num_proc=16):  10%|â–ˆ         | 101000/1000000 [00:04<00:32, 27347.22 examples/s]Running tokenizer on dataset (num_proc=16):  10%|â–ˆ         | 105000/1000000 [00:04<00:32, 27139.65 examples/s]Running tokenizer on dataset (num_proc=16):  11%|â–ˆ         | 109000/1000000 [00:05<00:34, 26132.77 examples/s]Running tokenizer on dataset (num_proc=16):  11%|â–ˆâ–        | 114000/1000000 [00:05<00:31, 28116.74 examples/s]Running tokenizer on dataset (num_proc=16):  12%|â–ˆâ–        | 117000/1000000 [00:05<00:32, 27210.34 examples/s]Running tokenizer on dataset (num_proc=16):  12%|â–ˆâ–        | 121000/1000000 [00:05<00:33, 26590.71 examples/s]Running tokenizer on dataset (num_proc=16):  12%|â–ˆâ–Ž        | 125000/1000000 [00:05<00:32, 26523.87 examples/s]Running tokenizer on dataset (num_proc=16):  13%|â–ˆâ–Ž        | 130000/1000000 [00:05<00:31, 27931.59 examples/s]Running tokenizer on dataset (num_proc=16):  13%|â–ˆâ–Ž        | 133000/1000000 [00:05<00:32, 26750.51 examples/s]Running tokenizer on dataset (num_proc=16):  14%|â–ˆâ–Ž        | 137000/1000000 [00:06<00:32, 26462.67 examples/s]Running tokenizer on dataset (num_proc=16):  14%|â–ˆâ–        | 141000/1000000 [00:06<00:32, 26754.91 examples/s]Running tokenizer on dataset (num_proc=16):  14%|â–ˆâ–        | 145000/1000000 [00:06<00:28, 29564.13 examples/s]Running tokenizer on dataset (num_proc=16):  15%|â–ˆâ–        | 149000/1000000 [00:06<00:31, 26708.75 examples/s]Running tokenizer on dataset (num_proc=16):  15%|â–ˆâ–Œ        | 153000/1000000 [00:06<00:32, 26034.07 examples/s]Running tokenizer on dataset (num_proc=16):  16%|â–ˆâ–Œ        | 157000/1000000 [00:06<00:31, 26759.38 examples/s]Running tokenizer on dataset (num_proc=16):  16%|â–ˆâ–Œ        | 162000/1000000 [00:06<00:29, 28480.88 examples/s]Running tokenizer on dataset (num_proc=16):  16%|â–ˆâ–‹        | 165000/1000000 [00:07<00:31, 26660.18 examples/s]Running tokenizer on dataset (num_proc=16):  17%|â–ˆâ–‹        | 169000/1000000 [00:07<00:31, 26123.70 examples/s]Running tokenizer on dataset (num_proc=16):  17%|â–ˆâ–‹        | 172000/1000000 [00:07<00:30, 26931.47 examples/s]Running tokenizer on dataset (num_proc=16):  18%|â–ˆâ–Š        | 176000/1000000 [00:07<00:28, 29039.38 examples/s]Running tokenizer on dataset (num_proc=16):  18%|â–ˆâ–Š        | 180000/1000000 [00:07<00:31, 26331.57 examples/s]Running tokenizer on dataset (num_proc=16):  18%|â–ˆâ–Š        | 184000/1000000 [00:07<00:29, 27308.61 examples/s]Running tokenizer on dataset (num_proc=16):  19%|â–ˆâ–‰        | 188000/1000000 [00:07<00:31, 25624.27 examples/s]Running tokenizer on dataset (num_proc=16):  19%|â–ˆâ–‰        | 193000/1000000 [00:08<00:27, 29628.12 examples/s]Running tokenizer on dataset (num_proc=16):  20%|â–ˆâ–‰        | 197000/1000000 [00:08<00:31, 25525.90 examples/s]Running tokenizer on dataset (num_proc=16):  20%|â–ˆâ–ˆ        | 201000/1000000 [00:08<00:30, 26613.86 examples/s]Running tokenizer on dataset (num_proc=16):  20%|â–ˆâ–ˆ        | 204000/1000000 [00:08<00:30, 26326.75 examples/s]Running tokenizer on dataset (num_proc=16):  21%|â–ˆâ–ˆ        | 209000/1000000 [00:08<00:26, 29511.59 examples/s]Running tokenizer on dataset (num_proc=16):  21%|â–ˆâ–ˆâ–       | 213000/1000000 [00:08<00:30, 25508.72 examples/s]Running tokenizer on dataset (num_proc=16):  22%|â–ˆâ–ˆâ–       | 217000/1000000 [00:09<00:29, 26965.68 examples/s]Running tokenizer on dataset (num_proc=16):  22%|â–ˆâ–ˆâ–       | 220000/1000000 [00:09<00:28, 26920.02 examples/s]Running tokenizer on dataset (num_proc=16):  22%|â–ˆâ–ˆâ–       | 224000/1000000 [00:09<00:26, 29303.83 examples/s]Running tokenizer on dataset (num_proc=16):  23%|â–ˆâ–ˆâ–Ž       | 228000/1000000 [00:09<00:28, 27182.43 examples/s]Running tokenizer on dataset (num_proc=16):  23%|â–ˆâ–ˆâ–Ž       | 231000/1000000 [00:09<00:29, 25867.55 examples/s]Running tokenizer on dataset (num_proc=16):  24%|â–ˆâ–ˆâ–Ž       | 236000/1000000 [00:09<00:28, 26913.12 examples/s]Running tokenizer on dataset (num_proc=16):  24%|â–ˆâ–ˆâ–       | 240000/1000000 [00:09<00:26, 29022.23 examples/s]Running tokenizer on dataset (num_proc=16):  24%|â–ˆâ–ˆâ–       | 244000/1000000 [00:10<00:28, 26974.63 examples/s]Running tokenizer on dataset (num_proc=16):  25%|â–ˆâ–ˆâ–       | 247000/1000000 [00:10<00:28, 25966.85 examples/s]Running tokenizer on dataset (num_proc=16):  25%|â–ˆâ–ˆâ–Œ       | 252000/1000000 [00:10<00:27, 27479.53 examples/s]Running tokenizer on dataset (num_proc=16):  26%|â–ˆâ–ˆâ–Œ       | 256000/1000000 [00:10<00:26, 28477.03 examples/s]Running tokenizer on dataset (num_proc=16):  26%|â–ˆâ–ˆâ–Œ       | 260000/1000000 [00:10<00:27, 26750.58 examples/s]Running tokenizer on dataset (num_proc=16):  26%|â–ˆâ–ˆâ–‹       | 263000/1000000 [00:10<00:29, 24808.20 examples/s]Running tokenizer on dataset (num_proc=16):  27%|â–ˆâ–ˆâ–‹       | 268000/1000000 [00:10<00:26, 28102.73 examples/s]Running tokenizer on dataset (num_proc=16):  27%|â–ˆâ–ˆâ–‹       | 272000/1000000 [00:11<00:25, 28297.21 examples/s]Running tokenizer on dataset (num_proc=16):  28%|â–ˆâ–ˆâ–Š       | 276000/1000000 [00:11<00:26, 27252.63 examples/s]Running tokenizer on dataset (num_proc=16):  28%|â–ˆâ–ˆâ–Š       | 279000/1000000 [00:11<00:30, 23939.80 examples/s]Running tokenizer on dataset (num_proc=16):  28%|â–ˆâ–ˆâ–Š       | 284000/1000000 [00:11<00:24, 28912.53 examples/s]Running tokenizer on dataset (num_proc=16):  29%|â–ˆâ–ˆâ–‰       | 288000/1000000 [00:11<00:24, 28947.20 examples/s]Running tokenizer on dataset (num_proc=16):  29%|â–ˆâ–ˆâ–‰       | 292000/1000000 [00:11<00:26, 26768.34 examples/s]Running tokenizer on dataset (num_proc=16):  30%|â–ˆâ–ˆâ–‰       | 295000/1000000 [00:11<00:30, 23319.79 examples/s]Running tokenizer on dataset (num_proc=16):  30%|â–ˆâ–ˆâ–ˆ       | 301000/1000000 [00:12<00:24, 28013.43 examples/s]Running tokenizer on dataset (num_proc=16):  30%|â–ˆâ–ˆâ–ˆ       | 305000/1000000 [00:12<00:24, 28817.53 examples/s]Running tokenizer on dataset (num_proc=16):  31%|â–ˆâ–ˆâ–ˆ       | 308000/1000000 [00:12<00:25, 26904.03 examples/s]Running tokenizer on dataset (num_proc=16):  31%|â–ˆâ–ˆâ–ˆ       | 311000/1000000 [00:12<00:29, 23644.39 examples/s]Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 317000/1000000 [00:12<00:24, 27786.03 examples/s]Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 321000/1000000 [00:12<00:23, 29244.77 examples/s]Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 324000/1000000 [00:12<00:24, 27088.86 examples/s]Running tokenizer on dataset (num_proc=16):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 327000/1000000 [00:13<00:28, 23483.02 examples/s]Running tokenizer on dataset (num_proc=16):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 333000/1000000 [00:13<00:23, 28044.89 examples/s]Running tokenizer on dataset (num_proc=16):  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 337000/1000000 [00:13<00:22, 29775.98 examples/s]Running tokenizer on dataset (num_proc=16):  34%|â–ˆâ–ˆâ–ˆâ–      | 341000/1000000 [00:13<00:26, 25023.63 examples/s]Running tokenizer on dataset (num_proc=16):  34%|â–ˆâ–ˆâ–ˆâ–      | 344000/1000000 [00:13<00:25, 25523.18 examples/s]Running tokenizer on dataset (num_proc=16):  35%|â–ˆâ–ˆâ–ˆâ–      | 349000/1000000 [00:13<00:23, 28177.72 examples/s]Running tokenizer on dataset (num_proc=16):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353000/1000000 [00:14<00:21, 29650.44 examples/s]Running tokenizer on dataset (num_proc=16):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 357000/1000000 [00:14<00:25, 25378.04 examples/s]Running tokenizer on dataset (num_proc=16):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360000/1000000 [00:14<00:25, 24664.07 examples/s]Running tokenizer on dataset (num_proc=16):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 365000/1000000 [00:14<00:22, 28685.35 examples/s]Running tokenizer on dataset (num_proc=16):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 369000/1000000 [00:14<00:21, 29827.40 examples/s]Running tokenizer on dataset (num_proc=16):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 373000/1000000 [00:14<00:25, 25060.63 examples/s]Running tokenizer on dataset (num_proc=16):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 376000/1000000 [00:14<00:25, 24481.69 examples/s]Running tokenizer on dataset (num_proc=16):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 381000/1000000 [00:15<00:21, 29219.81 examples/s]Running tokenizer on dataset (num_proc=16):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 385000/1000000 [00:15<00:20, 29380.24 examples/s]Running tokenizer on dataset (num_proc=16):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 389000/1000000 [00:15<00:24, 25222.38 examples/s]Running tokenizer on dataset (num_proc=16):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 392000/1000000 [00:15<00:24, 24636.13 examples/s]Running tokenizer on dataset (num_proc=16):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 397000/1000000 [00:15<00:20, 29374.02 examples/s]Running tokenizer on dataset (num_proc=16):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 401000/1000000 [00:15<00:20, 29438.32 examples/s]Running tokenizer on dataset (num_proc=16):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 405000/1000000 [00:16<00:23, 25479.04 examples/s]Running tokenizer on dataset (num_proc=16):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 408000/1000000 [00:16<00:24, 24255.51 examples/s]Running tokenizer on dataset (num_proc=16):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 413000/1000000 [00:16<00:19, 29420.92 examples/s]Running tokenizer on dataset (num_proc=16):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 417000/1000000 [00:16<00:19, 29378.46 examples/s]Running tokenizer on dataset (num_proc=16):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 421000/1000000 [00:16<00:22, 25419.29 examples/s]Running tokenizer on dataset (num_proc=16):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 424000/1000000 [00:16<00:24, 23859.40 examples/s]Running tokenizer on dataset (num_proc=16):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430000/1000000 [00:16<00:21, 26325.79 examples/s]Running tokenizer on dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 436000/1000000 [00:17<00:19, 28365.96 examples/s]Running tokenizer on dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 439000/1000000 [00:17<00:23, 24349.33 examples/s]Running tokenizer on dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 443000/1000000 [00:17<00:20, 26691.16 examples/s]Running tokenizer on dataset (num_proc=16):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 446000/1000000 [00:17<00:20, 27041.12 examples/s]Running tokenizer on dataset (num_proc=16):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452000/1000000 [00:17<00:18, 29357.87 examples/s]Running tokenizer on dataset (num_proc=16):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 455000/1000000 [00:17<00:22, 24758.47 examples/s]Running tokenizer on dataset (num_proc=16):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 459000/1000000 [00:18<00:20, 26211.98 examples/s]Running tokenizer on dataset (num_proc=16):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 463000/1000000 [00:18<00:19, 28027.18 examples/s]Running tokenizer on dataset (num_proc=16):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 468000/1000000 [00:18<00:17, 29961.51 examples/s]Running tokenizer on dataset (num_proc=16):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 472000/1000000 [00:18<00:20, 26100.27 examples/s]Running tokenizer on dataset (num_proc=16):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 475000/1000000 [00:18<00:20, 25571.23 examples/s]Running tokenizer on dataset (num_proc=16):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 478000/1000000 [00:18<00:20, 25511.28 examples/s]Running tokenizer on dataset (num_proc=16):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 484000/1000000 [00:18<00:16, 30459.31 examples/s]Running tokenizer on dataset (num_proc=16):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 488000/1000000 [00:19<00:19, 26357.69 examples/s]Running tokenizer on dataset (num_proc=16):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 491000/1000000 [00:19<00:19, 25906.29 examples/s]Running tokenizer on dataset (num_proc=16):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 494000/1000000 [00:19<00:19, 25618.76 examples/s]Running tokenizer on dataset (num_proc=16):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 499000/1000000 [00:19<00:16, 30687.75 examples/s]Running tokenizer on dataset (num_proc=16):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503000/1000000 [00:19<00:19, 25431.72 examples/s]Running tokenizer on dataset (num_proc=16):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 506000/1000000 [00:19<00:19, 25381.58 examples/s]Running tokenizer on dataset (num_proc=16):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510000/1000000 [00:19<00:18, 26155.90 examples/s]Running tokenizer on dataset (num_proc=16):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515000/1000000 [00:20<00:15, 30327.24 examples/s]Running tokenizer on dataset (num_proc=16):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519000/1000000 [00:20<00:19, 25249.48 examples/s]Running tokenizer on dataset (num_proc=16):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 522000/1000000 [00:20<00:18, 25548.92 examples/s]Running tokenizer on dataset (num_proc=16):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 526000/1000000 [00:20<00:17, 26827.44 examples/s]Running tokenizer on dataset (num_proc=16):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530000/1000000 [00:20<00:16, 28960.06 examples/s]Running tokenizer on dataset (num_proc=16):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 534000/1000000 [00:20<00:16, 28228.96 examples/s]Running tokenizer on dataset (num_proc=16):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 537000/1000000 [00:20<00:19, 23918.94 examples/s]Running tokenizer on dataset (num_proc=16):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 541000/1000000 [00:21<00:16, 27415.21 examples/s]Running tokenizer on dataset (num_proc=16):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 544000/1000000 [00:21<00:16, 27598.29 examples/s]Running tokenizer on dataset (num_proc=16):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 548000/1000000 [00:21<00:14, 30524.77 examples/s]Running tokenizer on dataset (num_proc=16):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552000/1000000 [00:21<00:17, 26286.69 examples/s]Running tokenizer on dataset (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 555000/1000000 [00:21<00:17, 26118.06 examples/s]Running tokenizer on dataset (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 559000/1000000 [00:21<00:17, 24882.26 examples/s]Running tokenizer on dataset (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565000/1000000 [00:21<00:15, 28808.92 examples/s]Running tokenizer on dataset (num_proc=16):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 568000/1000000 [00:22<00:15, 27138.81 examples/s]Running tokenizer on dataset (num_proc=16):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 571000/1000000 [00:22<00:16, 25513.81 examples/s]Running tokenizer on dataset (num_proc=16):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 575000/1000000 [00:22<00:16, 25466.51 examples/s]Running tokenizer on dataset (num_proc=16):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 581000/1000000 [00:22<00:14, 29032.87 examples/s]Running tokenizer on dataset (num_proc=16):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 584000/1000000 [00:22<00:15, 27418.62 examples/s]Running tokenizer on dataset (num_proc=16):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 587000/1000000 [00:22<00:15, 25970.44 examples/s]Running tokenizer on dataset (num_proc=16):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 591000/1000000 [00:22<00:16, 25402.13 examples/s]Running tokenizer on dataset (num_proc=16):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 597000/1000000 [00:23<00:13, 29051.09 examples/s]Running tokenizer on dataset (num_proc=16):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600000/1000000 [00:23<00:14, 27091.27 examples/s]Running tokenizer on dataset (num_proc=16):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603000/1000000 [00:23<00:15, 26011.10 examples/s]Running tokenizer on dataset (num_proc=16):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 607000/1000000 [00:23<00:15, 25393.48 examples/s]Running tokenizer on dataset (num_proc=16):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 613000/1000000 [00:23<00:13, 29314.49 examples/s]Running tokenizer on dataset (num_proc=16):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 616000/1000000 [00:23<00:14, 26158.76 examples/s]Running tokenizer on dataset (num_proc=16):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 619000/1000000 [00:23<00:14, 26124.11 examples/s]Running tokenizer on dataset (num_proc=16):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 623000/1000000 [00:24<00:14, 25460.02 examples/s]Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 629000/1000000 [00:24<00:12, 29141.78 examples/s]Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 632000/1000000 [00:24<00:13, 26393.10 examples/s]Running tokenizer on dataset (num_proc=16):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 635000/1000000 [00:24<00:14, 25965.83 examples/s]Running tokenizer on dataset (num_proc=16):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 639000/1000000 [00:24<00:14, 25718.49 examples/s]Running tokenizer on dataset (num_proc=16):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 645000/1000000 [00:24<00:12, 29292.80 examples/s]Running tokenizer on dataset (num_proc=16):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 648000/1000000 [00:25<00:13, 26637.43 examples/s]Running tokenizer on dataset (num_proc=16):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 651000/1000000 [00:25<00:13, 25977.60 examples/s]Running tokenizer on dataset (num_proc=16):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 655000/1000000 [00:25<00:13, 25849.00 examples/s]Running tokenizer on dataset (num_proc=16):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 661000/1000000 [00:25<00:11, 29592.53 examples/s]Running tokenizer on dataset (num_proc=16):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 664000/1000000 [00:25<00:12, 26071.41 examples/s]Running tokenizer on dataset (num_proc=16):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 667000/1000000 [00:25<00:13, 25285.34 examples/s]Running tokenizer on dataset (num_proc=16):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 671000/1000000 [00:25<00:12, 25972.35 examples/s]Running tokenizer on dataset (num_proc=16):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 677000/1000000 [00:26<00:10, 29824.66 examples/s]Running tokenizer on dataset (num_proc=16):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680000/1000000 [00:26<00:12, 26188.52 examples/s]Running tokenizer on dataset (num_proc=16):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 683000/1000000 [00:26<00:12, 25480.35 examples/s]Running tokenizer on dataset (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 687000/1000000 [00:26<00:11, 26503.18 examples/s]Running tokenizer on dataset (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 692000/1000000 [00:26<00:09, 31686.13 examples/s]Running tokenizer on dataset (num_proc=16):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 696000/1000000 [00:26<00:11, 25675.94 examples/s]Running tokenizer on dataset (num_proc=16):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 699000/1000000 [00:26<00:11, 25400.13 examples/s]Running tokenizer on dataset (num_proc=16):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703000/1000000 [00:27<00:11, 26314.95 examples/s]Running tokenizer on dataset (num_proc=16):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 708000/1000000 [00:27<00:09, 30989.75 examples/s]Running tokenizer on dataset (num_proc=16):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 712000/1000000 [00:27<00:11, 25564.22 examples/s]Running tokenizer on dataset (num_proc=16):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 715000/1000000 [00:27<00:11, 25103.08 examples/s]Running tokenizer on dataset (num_proc=16):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 719000/1000000 [00:27<00:10, 26645.83 examples/s]Running tokenizer on dataset (num_proc=16):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 723000/1000000 [00:27<00:09, 29549.20 examples/s]Running tokenizer on dataset (num_proc=16):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 727000/1000000 [00:27<00:10, 26490.17 examples/s]Running tokenizer on dataset (num_proc=16):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730000/1000000 [00:28<00:11, 23658.22 examples/s]Running tokenizer on dataset (num_proc=16):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 734000/1000000 [00:28<00:09, 27115.72 examples/s]Running tokenizer on dataset (num_proc=16):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 738000/1000000 [00:28<00:09, 28413.73 examples/s]Running tokenizer on dataset (num_proc=16):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 742000/1000000 [00:28<00:08, 30270.59 examples/s]Running tokenizer on dataset (num_proc=16):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 746000/1000000 [00:28<00:10, 23984.53 examples/s]Running tokenizer on dataset (num_proc=16):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750000/1000000 [00:28<00:09, 27119.97 examples/s]Running tokenizer on dataset (num_proc=16):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 754000/1000000 [00:28<00:08, 28021.71 examples/s]Running tokenizer on dataset (num_proc=16):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 758000/1000000 [00:29<00:08, 28859.14 examples/s]Running tokenizer on dataset (num_proc=16):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 762000/1000000 [00:29<00:09, 24403.21 examples/s]Running tokenizer on dataset (num_proc=16):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 766000/1000000 [00:29<00:08, 26929.71 examples/s]Running tokenizer on dataset (num_proc=16):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770000/1000000 [00:29<00:08, 28514.20 examples/s]Running tokenizer on dataset (num_proc=16):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774000/1000000 [00:29<00:07, 28641.90 examples/s]Running tokenizer on dataset (num_proc=16):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 778000/1000000 [00:29<00:09, 24638.56 examples/s]Running tokenizer on dataset (num_proc=16):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 782000/1000000 [00:30<00:08, 26517.73 examples/s]Running tokenizer on dataset (num_proc=16):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 786000/1000000 [00:30<00:07, 28480.92 examples/s]Running tokenizer on dataset (num_proc=16):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790000/1000000 [00:30<00:07, 28299.53 examples/s]Running tokenizer on dataset (num_proc=16):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 793000/1000000 [00:30<00:08, 24338.60 examples/s]Running tokenizer on dataset (num_proc=16):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 798000/1000000 [00:30<00:07, 26529.66 examples/s]Running tokenizer on dataset (num_proc=16):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802000/1000000 [00:30<00:07, 27994.00 examples/s]Running tokenizer on dataset (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 806000/1000000 [00:30<00:06, 27841.88 examples/s]Running tokenizer on dataset (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 809000/1000000 [00:31<00:07, 25101.34 examples/s]Running tokenizer on dataset (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 813000/1000000 [00:31<00:06, 27538.88 examples/s]Running tokenizer on dataset (num_proc=16):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 816000/1000000 [00:31<00:06, 27998.59 examples/s]Running tokenizer on dataset (num_proc=16):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820000/1000000 [00:31<00:05, 30762.28 examples/s]Running tokenizer on dataset (num_proc=16):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 824000/1000000 [00:31<00:07, 24454.43 examples/s]Running tokenizer on dataset (num_proc=16):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 827000/1000000 [00:31<00:06, 25003.34 examples/s]Running tokenizer on dataset (num_proc=16):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 831000/1000000 [00:31<00:06, 26935.65 examples/s]Running tokenizer on dataset (num_proc=16):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 836000/1000000 [00:31<00:05, 30278.63 examples/s]Running tokenizer on dataset (num_proc=16):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840000/1000000 [00:32<00:06, 25019.94 examples/s]Running tokenizer on dataset (num_proc=16):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 843000/1000000 [00:32<00:06, 25567.82 examples/s]Running tokenizer on dataset (num_proc=16):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 847000/1000000 [00:32<00:05, 27203.52 examples/s]Running tokenizer on dataset (num_proc=16):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 851000/1000000 [00:32<00:04, 30095.96 examples/s]Running tokenizer on dataset (num_proc=16):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 855000/1000000 [00:32<00:05, 25898.75 examples/s]Running tokenizer on dataset (num_proc=16):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 858000/1000000 [00:32<00:05, 25717.11 examples/s]Running tokenizer on dataset (num_proc=16):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 862000/1000000 [00:32<00:05, 27329.40 examples/s]Running tokenizer on dataset (num_proc=16):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 865000/1000000 [00:33<00:05, 26610.79 examples/s]Running tokenizer on dataset (num_proc=16):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870000/1000000 [00:33<00:05, 24916.77 examples/s]Running tokenizer on dataset (num_proc=16):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 874000/1000000 [00:33<00:05, 24681.39 examples/s]Running tokenizer on dataset (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 877000/1000000 [00:33<00:04, 24733.35 examples/s]Running tokenizer on dataset (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 881000/1000000 [00:33<00:04, 25700.13 examples/s]Running tokenizer on dataset (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 884000/1000000 [00:33<00:04, 23504.05 examples/s]Running tokenizer on dataset (num_proc=16):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 887000/1000000 [00:34<00:05, 21668.48 examples/s]Running tokenizer on dataset (num_proc=16):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890000/1000000 [00:34<00:04, 23086.10 examples/s]Running tokenizer on dataset (num_proc=16):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 893000/1000000 [00:34<00:04, 22142.73 examples/s]Running tokenizer on dataset (num_proc=16):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 897000/1000000 [00:34<00:04, 23416.28 examples/s]Running tokenizer on dataset (num_proc=16):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902000/1000000 [00:34<00:03, 27609.49 examples/s]Running tokenizer on dataset (num_proc=16):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 905000/1000000 [00:34<00:03, 24310.47 examples/s]Running tokenizer on dataset (num_proc=16):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 908000/1000000 [00:34<00:03, 23412.82 examples/s]Running tokenizer on dataset (num_proc=16):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 913000/1000000 [00:35<00:03, 26141.95 examples/s]Running tokenizer on dataset (num_proc=16):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 918000/1000000 [00:35<00:02, 30055.04 examples/s]Running tokenizer on dataset (num_proc=16):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 922000/1000000 [00:35<00:03, 25622.74 examples/s]Running tokenizer on dataset (num_proc=16):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 925000/1000000 [00:35<00:02, 25715.64 examples/s]Running tokenizer on dataset (num_proc=16):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 929000/1000000 [00:35<00:02, 26286.77 examples/s]Running tokenizer on dataset (num_proc=16):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 934000/1000000 [00:35<00:02, 30193.89 examples/s]Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938000/1000000 [00:36<00:02, 25475.38 examples/s]Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 941000/1000000 [00:36<00:02, 25808.58 examples/s]Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 945000/1000000 [00:36<00:02, 26202.56 examples/s]Running tokenizer on dataset (num_proc=16):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950000/1000000 [00:36<00:01, 30467.84 examples/s]Running tokenizer on dataset (num_proc=16):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 954000/1000000 [00:36<00:01, 25615.50 examples/s]Running tokenizer on dataset (num_proc=16):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 957000/1000000 [00:36<00:01, 25665.49 examples/s]Running tokenizer on dataset (num_proc=16):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 961000/1000000 [00:36<00:01, 26477.95 examples/s]Running tokenizer on dataset (num_proc=16):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 966000/1000000 [00:36<00:01, 31094.04 examples/s]Running tokenizer on dataset (num_proc=16):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970000/1000000 [00:37<00:01, 23687.20 examples/s]Running tokenizer on dataset (num_proc=16):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 975500/1000000 [00:37<00:00, 28480.06 examples/s]Running tokenizer on dataset (num_proc=16):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 979500/1000000 [00:37<00:00, 28633.19 examples/s]Running tokenizer on dataset (num_proc=16):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 983500/1000000 [00:37<00:00, 25678.74 examples/s]Running tokenizer on dataset (num_proc=16):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 986500/1000000 [00:37<00:00, 26275.87 examples/s]Running tokenizer on dataset (num_proc=16):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990000/1000000 [00:37<00:00, 27017.74 examples/s]Running tokenizer on dataset (num_proc=16):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 993000/1000000 [00:38<00:00, 26614.29 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 996000/1000000 [00:38<00:00, 23432.81 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 999000/1000000 [00:38<00:00, 21612.71 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [00:44<00:00, 22546.82 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 5501, 6923, 220, 16, 18, 2410, 11203, 1056, 304, 6364, 4128, 11, 3118, 389, 279, 2701, 1995, 1447, 19357, 2864, 25, 8438, 836, 33063, 905, 715, 13636, 25, 8647, 84, 905, 715, 6746, 25, 35324, 82, 1177, 758, 60594, 35324, 82, 1177, 758, 60594, 8211, 2576, 1177, 91379, 577, 715, 5009, 2124, 11859, 261, 25, 8647, 84, 374, 264, 3644, 5339, 12938, 553, 393, 4103, 53542, 429, 6081, 47674, 11561, 13, 715, 12404, 16527, 25, 1948, 220, 16, 15, 311, 220, 18, 15, 5766, 13, 715, 151645, 198, 151644, 77091, 198, 2589, 81268, 858, 466, 10684, 25, 8647, 84, 198, 2589, 25, 17482, 6601, 304, 264, 9189, 198, 2589, 25, 50002, 5633, 28650, 19410, 198, 2589, 25, 5366, 54232, 16136, 198, 2589, 25, 4431, 8105, 11201, 198, 2589, 25, 44795, 3018, 41182, 11, 4599, 369, 1446, 198, 2589, 25, 43824, 594, 85521, 24145, 198, 2589, 25, 46432, 5633, 11101, 4695, 198, 2589, 25, 21988, 84, 25, 18609, 57679, 198, 2589, 25, 11976, 28242, 10733, 518, 12041, 36033, 198, 2589, 25, 21396, 51318, 11, 425, 4500, 57679, 198, 2589, 25, 8784, 6164, 3085, 8647, 84, 198, 2589, 25, 50002, 19207, 1506, 57679, 198, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
Please generate 13 Ad Headline in English language, based on the following information:

FinalUrl: www.Temu.com 
Domain: Temu.com 
Category: Chemicals -- Inorganic Chemicals -- Inorganic Salts -- Chloride 
DescriptionOfAdvertiser: Temu is a global platform owned by PDD Holdings that offers discounted goods. 
CharacterLimit: between 10 to 30 characters. 
<|im_end|>
<|im_start|>assistant
Ad:Bargain Central: Temu
Ad:Buy Button in a Click
Ad:Discover Our Unique Items
Ad:Top Brands Available
Ad:Order Online Today
Ad:Tailored Deals, Just for You
Ad:Earth's Biggest Selection
Ad:Browse Our Collection Now
Ad:Temu: Easy Savings
Ad:High-Quality at Low Prices
Ad:Great Offers, Bigger Savings
Ad:Save Big With Temu
Ad:Discover Personalized Savings
<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2589, 81268, 858, 466, 10684, 25, 8647, 84, 198, 2589, 25, 17482, 6601, 304, 264, 9189, 198, 2589, 25, 50002, 5633, 28650, 19410, 198, 2589, 25, 5366, 54232, 16136, 198, 2589, 25, 4431, 8105, 11201, 198, 2589, 25, 44795, 3018, 41182, 11, 4599, 369, 1446, 198, 2589, 25, 43824, 594, 85521, 24145, 198, 2589, 25, 46432, 5633, 11101, 4695, 198, 2589, 25, 21988, 84, 25, 18609, 57679, 198, 2589, 25, 11976, 28242, 10733, 518, 12041, 36033, 198, 2589, 25, 21396, 51318, 11, 425, 4500, 57679, 198, 2589, 25, 8784, 6164, 3085, 8647, 84, 198, 2589, 25, 50002, 19207, 1506, 57679, 198, 151645]
labels:
Ad:Bargain Central: Temu
Ad:Buy Button in a Click
Ad:Discover Our Unique Items
Ad:Top Brands Available
Ad:Order Online Today
Ad:Tailored Deals, Just for You
Ad:Earth's Biggest Selection
Ad:Browse Our Collection Now
Ad:Temu: Easy Savings
Ad:High-Quality at Low Prices
Ad:Great Offers, Bigger Savings
Ad:Save Big With Temu
Ad:Discover Personalized Savings
<|im_end|>
[INFO|configuration_utils.py:731] 2024-07-25 03:54:55,485 >> loading configuration file /data/xiaoyukou/ckpts/Qwen2-7B-Instruct/config.json
[INFO|configuration_utils.py:796] 2024-07-25 03:54:55,486 >> Model config Qwen2Config {
  "_name_or_path": "/data/xiaoyukou/ckpts/Qwen2-7B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

07/25/2024 03:54:55 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
[INFO|modeling_utils.py:3471] 2024-07-25 03:54:55,500 >> loading weights file /data/xiaoyukou/ckpts/Qwen2-7B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1519] 2024-07-25 03:54:55,500 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:962] 2024-07-25 03:54:55,501 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

07/25/2024 03:54:55 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:02,  1.13it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:02,  1.15it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.13it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.14it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:02<00:00,  1.11it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:02<00:00,  1.11it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]
[INFO|modeling_utils.py:4280] 2024-07-25 03:55:00,293 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4288] 2024-07-25 03:55:00,293 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data/xiaoyukou/ckpts/Qwen2-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:915] 2024-07-25 03:55:00,295 >> loading configuration file /data/xiaoyukou/ckpts/Qwen2-7B-Instruct/generation_config.json
[INFO|configuration_utils.py:962] 2024-07-25 03:55:00,295 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

07/25/2024 03:55:00 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/25/2024 03:55:00 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
07/25/2024 03:55:00 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
07/25/2024 03:55:00 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/25/2024 03:55:00 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,down_proj,q_proj,up_proj,k_proj,o_proj,gate_proj
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.20it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.17it/s]
07/25/2024 03:55:00 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/25/2024 03:55:00 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
07/25/2024 03:55:00 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
07/25/2024 03:55:00 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/25/2024 03:55:00 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,o_proj,v_proj,down_proj,k_proj,up_proj,gate_proj
07/25/2024 03:55:00 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
07/25/2024 03:55:00 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
[INFO|trainer.py:641] 2024-07-25 03:55:00,950 >> Using auto half precision backend
07/25/2024 03:55:00 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.
[INFO|trainer.py:2078] 2024-07-25 03:55:03,139 >> ***** Running training *****
[INFO|trainer.py:2079] 2024-07-25 03:55:03,139 >>   Num examples = 998,000
[INFO|trainer.py:2080] 2024-07-25 03:55:03,139 >>   Num Epochs = 1
[INFO|trainer.py:2081] 2024-07-25 03:55:03,139 >>   Instantaneous batch size per device = 20
[INFO|trainer.py:2084] 2024-07-25 03:55:03,139 >>   Total train batch size (w. parallel, distributed & accumulation) = 80
[INFO|trainer.py:2085] 2024-07-25 03:55:03,139 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2086] 2024-07-25 03:55:03,139 >>   Total optimization steps = 12,475
[INFO|trainer.py:2087] 2024-07-25 03:55:03,143 >>   Number of trainable parameters = 10,092,544
  0%|          | 0/12475 [00:00<?, ?it/s]  0%|          | 1/12475 [00:18<62:33:28, 18.05s/it]  0%|          | 2/12475 [00:30<51:52:00, 14.97s/it]  0%|          | 3/12475 [00:42<46:03:01, 13.29s/it]  0%|          | 4/12475 [00:55<45:54:37, 13.25s/it]  0%|          | 5/12475 [01:10<48:50:08, 14.10s/it]  0%|          | 6/12475 [01:26<50:36:58, 14.61s/it]  0%|          | 7/12475 [01:40<49:19:33, 14.24s/it]  0%|          | 8/12475 [01:55<50:54:34, 14.70s/it]  0%|          | 9/12475 [02:08<48:44:35, 14.08s/it]  0%|          | 10/12475 [02:20<46:27:02, 13.42s/it]                                                     {'loss': 2.901, 'grad_norm': 1.230023741722107, 'learning_rate': 9.999984460545386e-05, 'epoch': 0.0}
  0%|          | 10/12475 [02:20<46:27:02, 13.42s/it]  0%|          | 11/12475 [02:35<48:13:30, 13.93s/it]  0%|          | 12/12475 [02:48<47:29:52, 13.72s/it]  0%|          | 13/12475 [03:02<48:00:07, 13.87s/it]  0%|          | 14/12475 [03:16<47:30:41, 13.73s/it]  0%|          | 15/12475 [03:30<47:54:11, 13.84s/it]  0%|          | 16/12475 [03:42<46:30:06, 13.44s/it]  0%|          | 17/12475 [03:58<48:52:03, 14.12s/it]  0%|          | 18/12475 [04:11<47:24:15, 13.70s/it]  0%|          | 19/12475 [04:24<46:31:39, 13.45s/it]  0%|          | 20/12475 [04:35<44:31:36, 12.87s/it]                                                     {'loss': 1.9674, 'grad_norm': 0.46491432189941406, 'learning_rate': 9.99993721283818e-05, 'epoch': 0.0}
  0%|          | 20/12475 [04:35<44:31:36, 12.87s/it]  0%|          | 21/12475 [04:48<43:59:52, 12.72s/it]  0%|          | 22/12475 [05:00<44:07:35, 12.76s/it]  0%|          | 23/12475 [05:13<43:59:35, 12.72s/it]  0%|          | 24/12475 [05:26<44:33:05, 12.88s/it]  0%|          | 25/12475 [05:40<45:11:51, 13.07s/it]  0%|          | 26/12475 [05:51<43:43:23, 12.64s/it]  0%|          | 27/12475 [06:06<45:19:27, 13.11s/it]  0%|          | 28/12475 [06:20<46:16:09, 13.38s/it]  0%|          | 29/12475 [06:33<46:31:05, 13.46s/it]  0%|          | 30/12475 [06:49<48:30:51, 14.03s/it]                                                     {'loss': 1.8413, 'grad_norm': 0.37149766087532043, 'learning_rate': 9.999858255592527e-05, 'epoch': 0.0}
  0%|          | 30/12475 [06:49<48:30:51, 14.03s/it]  0%|          | 31/12475 [07:01<46:45:18, 13.53s/it]  0%|          | 32/12475 [07:13<44:38:46, 12.92s/it]  0%|          | 33/12475 [07:25<44:23:50, 12.85s/it]  0%|          | 34/12475 [07:40<45:54:03, 13.28s/it]  0%|          | 35/12475 [07:56<48:59:01, 14.18s/it]  0%|          | 36/12475 [08:11<49:56:27, 14.45s/it]  0%|          | 37/12475 [08:24<48:13:28, 13.96s/it]  0%|          | 38/12475 [08:45<55:53:55, 16.18s/it]  0%|          | 39/12475 [08:59<53:07:46, 15.38s/it]  0%|          | 40/12475 [09:12<51:00:39, 14.77s/it]                                                     {'loss': 1.8112, 'grad_norm': 0.3644358515739441, 'learning_rate': 9.999747589309174e-05, 'epoch': 0.0}
  0%|          | 40/12475 [09:12<51:00:39, 14.77s/it]  0%|          | 41/12475 [09:25<48:51:16, 14.14s/it]  0%|          | 42/12475 [09:37<47:21:12, 13.71s/it]  0%|          | 43/12475 [09:52<48:53:37, 14.16s/it]  0%|          | 44/12475 [10:04<46:20:30, 13.42s/it]  0%|          | 45/12475 [10:19<47:33:19, 13.77s/it]  0%|          | 46/12475 [10:31<45:38:36, 13.22s/it]  0%|          | 47/12475 [10:43<45:06:04, 13.06s/it]  0%|          | 48/12475 [10:57<45:22:59, 13.15s/it]  0%|          | 49/12475 [11:10<45:59:12, 13.32s/it]  0%|          | 50/12475 [11:27<48:55:08, 14.17s/it]                                                     {'loss': 1.8076, 'grad_norm': 0.3807903528213501, 'learning_rate': 9.999605214689967e-05, 'epoch': 0.0}
  0%|          | 50/12475 [11:27<48:55:08, 14.17s/it]  0%|          | 51/12475 [11:38<46:14:46, 13.40s/it]  0%|          | 52/12475 [11:52<46:34:10, 13.50s/it]  0%|          | 53/12475 [12:07<47:44:01, 13.83s/it]  0%|          | 54/12475 [12:19<46:10:31, 13.38s/it]  0%|          | 55/12475 [12:33<47:10:43, 13.67s/it]  0%|          | 56/12475 [12:46<45:44:48, 13.26s/it]  0%|          | 57/12475 [12:59<45:51:27, 13.29s/it]  0%|          | 58/12475 [13:14<48:04:49, 13.94s/it]  0%|          | 59/12475 [13:28<47:13:26, 13.69s/it]  0%|          | 60/12475 [13:41<47:17:16, 13.71s/it]                                                     {'loss': 1.7578, 'grad_norm': 0.3696335554122925, 'learning_rate': 9.999431132637839e-05, 'epoch': 0.0}
  0%|          | 60/12475 [13:41<47:17:16, 13.71s/it]  0%|          | 61/12475 [13:53<44:49:51, 13.00s/it]  0%|          | 62/12475 [14:05<43:56:17, 12.74s/it]  1%|          | 63/12475 [14:17<43:40:14, 12.67s/it]  1%|          | 64/12475 [14:35<49:20:47, 14.31s/it]  1%|          | 65/12475 [14:50<50:06:19, 14.54s/it]  1%|          | 66/12475 [15:03<48:25:30, 14.05s/it]  1%|          | 67/12475 [15:17<48:11:02, 13.98s/it]  1%|          | 68/12475 [15:33<49:47:07, 14.45s/it]  1%|          | 69/12475 [15:47<49:15:22, 14.29s/it]  1%|          | 70/12475 [15:59<47:42:39, 13.85s/it]                                                     {'loss': 1.7547, 'grad_norm': 0.3872316777706146, 'learning_rate': 9.999225344256824e-05, 'epoch': 0.01}
  1%|          | 70/12475 [15:59<47:42:39, 13.85s/it]  1%|          | 71/12475 [16:13<47:43:36, 13.85s/it]  1%|          | 72/12475 [16:28<48:59:36, 14.22s/it]  1%|          | 73/12475 [16:41<47:37:20, 13.82s/it]  1%|          | 74/12475 [16:54<46:38:39, 13.54s/it]  1%|          | 75/12475 [17:08<46:45:02, 13.57s/it]  1%|          | 76/12475 [17:21<46:29:07, 13.50s/it]  1%|          | 77/12475 [17:33<44:40:18, 12.97s/it]  1%|          | 78/12475 [17:46<44:20:24, 12.88s/it]  1%|          | 79/12475 [17:57<42:22:26, 12.31s/it]  1%|          | 80/12475 [18:10<43:25:06, 12.61s/it]                                                     {'loss': 1.7339, 'grad_norm': 0.4757373034954071, 'learning_rate': 9.998987850852022e-05, 'epoch': 0.01}
  1%|          | 80/12475 [18:10<43:25:06, 12.61s/it]  1%|          | 81/12475 [18:22<42:31:46, 12.35s/it]  1%|          | 82/12475 [18:35<43:44:48, 12.71s/it]  1%|          | 83/12475 [18:48<44:08:13, 12.82s/it]  1%|          | 84/12475 [19:01<44:22:11, 12.89s/it]  1%|          | 85/12475 [19:13<43:36:10, 12.67s/it]  1%|          | 86/12475 [19:27<44:06:51, 12.82s/it]  1%|          | 87/12475 [19:40<44:56:18, 13.06s/it]  1%|          | 88/12475 [19:56<47:25:04, 13.78s/it]  1%|          | 89/12475 [20:10<47:43:23, 13.87s/it]  1%|          | 90/12475 [20:25<49:01:20, 14.25s/it]                                                     {'loss': 1.7581, 'grad_norm': 0.46117329597473145, 'learning_rate': 9.998718653929617e-05, 'epoch': 0.01}
  1%|          | 90/12475 [20:25<49:01:20, 14.25s/it]  1%|          | 91/12475 [20:38<47:24:30, 13.78s/it]  1%|          | 92/12475 [20:51<46:52:04, 13.63s/it]  1%|          | 93/12475 [21:07<49:50:47, 14.49s/it]  1%|          | 94/12475 [21:21<49:20:37, 14.35s/it]  1%|          | 95/12475 [21:35<48:24:55, 14.08s/it]  1%|          | 96/12475 [21:51<50:18:35, 14.63s/it]  1%|          | 97/12475 [22:03<48:00:14, 13.96s/it]  1%|          | 98/12475 [22:18<48:50:01, 14.20s/it]  1%|          | 99/12475 [22:34<50:33:31, 14.71s/it]  1%|          | 100/12475 [22:48<50:04:46, 14.57s/it]                                                      {'loss': 1.737, 'grad_norm': 0.5070835947990417, 'learning_rate': 9.99841775519685e-05, 'epoch': 0.01}
  1%|          | 100/12475 [22:48<50:04:46, 14.57s/it]  1%|          | 101/12475 [23:03<49:59:48, 14.55s/it]  1%|          | 102/12475 [23:20<53:02:34, 15.43s/it]  1%|          | 103/12475 [23:34<51:19:18, 14.93s/it]  1%|          | 104/12475 [23:49<51:51:14, 15.09s/it]  1%|          | 105/12475 [24:03<50:07:40, 14.59s/it]  1%|          | 106/12475 [24:18<51:01:18, 14.85s/it]  1%|          | 107/12475 [24:30<48:23:08, 14.08s/it]  1%|          | 108/12475 [24:45<49:18:01, 14.35s/it]  1%|          | 109/12475 [24:59<49:01:48, 14.27s/it]  1%|          | 110/12475 [25:14<48:49:07, 14.21s/it]                                                      {'loss': 1.7019, 'grad_norm': 0.4975961744785309, 'learning_rate': 9.99808515656202e-05, 'epoch': 0.01}
  1%|          | 110/12475 [25:14<48:49:07, 14.21s/it]  1%|          | 111/12475 [25:27<47:54:18, 13.95s/it]  1%|          | 112/12475 [25:40<46:43:01, 13.60s/it]  1%|          | 113/12475 [25:51<44:22:02, 12.92s/it]  1%|          | 114/12475 [26:05<45:24:29, 13.22s/it]  1%|          | 115/12475 [26:19<46:12:15, 13.46s/it]  1%|          | 116/12475 [26:31<44:35:48, 12.99s/it]  1%|          | 117/12475 [26:43<43:53:37, 12.79s/it]  1%|          | 118/12475 [26:55<43:02:06, 12.54s/it]  1%|          | 119/12475 [27:06<41:04:50, 11.97s/it]  1%|          | 120/12475 [27:22<45:01:38, 13.12s/it]                                                      {'loss': 1.7208, 'grad_norm': 0.5309619307518005, 'learning_rate': 9.997720860134461e-05, 'epoch': 0.01}
  1%|          | 120/12475 [27:22<45:01:38, 13.12s/it]  1%|          | 121/12475 [27:36<45:57:29, 13.39s/it]  1%|          | 122/12475 [27:48<44:53:25, 13.08s/it]  1%|          | 123/12475 [28:00<44:03:13, 12.84s/it]  1%|          | 124/12475 [28:14<44:51:12, 13.07s/it]  1%|          | 125/12475 [28:28<45:39:30, 13.31s/it]  1%|          | 126/12475 [28:41<45:17:30, 13.20s/it]  1%|          | 127/12475 [28:54<45:00:09, 13.12s/it]  1%|          | 128/12475 [29:05<43:21:17, 12.64s/it]  1%|          | 129/12475 [29:18<43:45:16, 12.76s/it]  1%|          | 130/12475 [29:32<44:21:54, 12.94s/it]                                                      {'loss': 1.689, 'grad_norm': 0.5634170174598694, 'learning_rate': 9.997324868224538e-05, 'epoch': 0.01}
  1%|          | 130/12475 [29:32<44:21:54, 12.94s/it]  1%|          | 131/12475 [29:46<45:56:10, 13.40s/it]  1%|          | 132/12475 [30:00<46:26:12, 13.54s/it]  1%|          | 133/12475 [30:13<45:34:11, 13.29s/it]  1%|          | 134/12475 [30:27<47:07:57, 13.75s/it]  1%|          | 135/12475 [30:41<47:29:50, 13.86s/it]  1%|          | 136/12475 [30:56<48:22:06, 14.11s/it]  1%|          | 137/12475 [31:08<45:42:41, 13.34s/it]  1%|          | 138/12475 [31:20<44:37:45, 13.02s/it]  1%|          | 139/12475 [31:32<44:00:49, 12.84s/it]  1%|          | 140/12475 [31:47<45:52:51, 13.39s/it]                                                      {'loss': 1.7015, 'grad_norm': 0.5552194714546204, 'learning_rate': 9.996897183343624e-05, 'epoch': 0.01}
  1%|          | 140/12475 [31:47<45:52:51, 13.39s/it]  1%|          | 141/12475 [32:02<47:21:30, 13.82s/it]  1%|          | 142/12475 [32:16<47:40:40, 13.92s/it]  1%|          | 143/12475 [32:27<45:01:50, 13.15s/it]  1%|          | 144/12475 [32:40<44:19:42, 12.94s/it]  1%|          | 145/12475 [32:53<44:08:33, 12.89s/it]  1%|          | 146/12475 [33:05<43:34:03, 12.72s/it]  1%|          | 147/12475 [33:20<46:18:21, 13.52s/it]